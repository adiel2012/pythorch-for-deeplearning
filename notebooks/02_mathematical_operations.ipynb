{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/your-username/pytorch-for-deeplearning/blob/main/notebooks/02_mathematical_operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Chapter 2: Mathematical Operations\n",
    "\n",
    "This notebook explores advanced mathematical operations and activation functions in PyTorch.\n",
    "\n",
    "## Learning Objectives\n",
    "- Perform advanced mathematical operations on tensors\n",
    "- Understand and implement activation functions\n",
    "- Visualize activation function behaviors\n",
    "- Compare different activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install and import necessary libraries\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    !pip install torch torchvision torchaudio\n",
    "    import torch\n",
    "    print(f\"PyTorch installed. Version: {torch.__version__}\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set device and random seed\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "basic_math"
   },
   "source": [
    "## 1. Advanced Mathematical Operations\n",
    "\n",
    "Beyond basic arithmetic, PyTorch provides many mathematical functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "math_functions"
   },
   "outputs": [],
   "source": [
    "# Create sample tensor\n",
    "x = torch.linspace(-2, 2, 10)\n",
    "print(f\"Input tensor: {x}\")\n",
    "\n",
    "print(\"\\n=== Mathematical Functions ===\")\n",
    "# Trigonometric functions\n",
    "print(f\"sin(x): {torch.sin(x)}\")\n",
    "print(f\"cos(x): {torch.cos(x)}\")\n",
    "print(f\"tan(x): {torch.tan(x)}\")\n",
    "\n",
    "# Exponential and logarithmic functions\n",
    "x_pos = torch.linspace(0.1, 2, 5)  # Positive values for log\n",
    "print(f\"\\nPositive input: {x_pos}\")\n",
    "print(f\"exp(x): {torch.exp(x_pos)}\")\n",
    "print(f\"log(x): {torch.log(x_pos)}\")\n",
    "print(f\"log10(x): {torch.log10(x_pos)}\")\n",
    "print(f\"sqrt(x): {torch.sqrt(x_pos)}\")\n",
    "\n",
    "# Power functions\n",
    "print(f\"\\nPower functions:\")\n",
    "print(f\"x^2: {torch.pow(x, 2)}\")\n",
    "print(f\"x^3: {torch.pow(x, 3)}\")\n",
    "print(f\"2^x: {torch.pow(2, x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "linear_algebra"
   },
   "source": [
    "## 2. Linear Algebra Operations\n",
    "\n",
    "PyTorch provides comprehensive linear algebra operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "linalg_operations"
   },
   "outputs": [],
   "source": [
    "# Matrix operations\n",
    "A = torch.randn(3, 3)\n",
    "B = torch.randn(3, 3)\n",
    "\n",
    "print(f\"Matrix A:\\n{A}\")\n",
    "print(f\"\\nMatrix B:\\n{B}\")\n",
    "\n",
    "# Basic operations\n",
    "print(f\"\\n=== Linear Algebra Operations ===\")\n",
    "print(f\"Matrix multiplication (A @ B):\\n{A @ B}\")\n",
    "print(f\"\\nElement-wise multiplication (A * B):\\n{A * B}\")\n",
    "\n",
    "# Advanced operations\n",
    "print(f\"\\nDeterminant of A: {torch.linalg.det(A)}\")\n",
    "print(f\"Trace of A: {torch.trace(A)}\")\n",
    "print(f\"Frobenius norm of A: {torch.linalg.norm(A, 'fro')}\")\n",
    "\n",
    "# Eigenvalues and eigenvectors\n",
    "eigenvals, eigenvecs = torch.linalg.eig(A)\n",
    "print(f\"\\nEigenvalues: {eigenvals}\")\n",
    "print(f\"Eigenvectors shape: {eigenvecs.shape}\")\n",
    "\n",
    "# Matrix inverse (if invertible)\n",
    "try:\n",
    "    A_inv = torch.linalg.inv(A)\n",
    "    print(f\"\\nMatrix inverse exists\")\n",
    "    print(f\"A @ A_inv ≈ I: {torch.allclose(A @ A_inv, torch.eye(3), atol=1e-6)}\")\n",
    "except RuntimeError:\n",
    "    print(\"\\nMatrix is not invertible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "activation_functions"
   },
   "source": [
    "## 3. Activation Functions\n",
    "\n",
    "Activation functions are crucial in neural networks. Let's explore the most common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "basic_activations"
   },
   "outputs": [],
   "source": [
    "# Create input range for activation functions\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "\n",
    "# Define activation functions\n",
    "activations = {\n",
    "    'ReLU': torch.relu(x),\n",
    "    'Sigmoid': torch.sigmoid(x),\n",
    "    'Tanh': torch.tanh(x),\n",
    "    'LeakyReLU': F.leaky_relu(x, 0.1),\n",
    "    'ELU': F.elu(x),\n",
    "    'GELU': F.gelu(x),\n",
    "    'SiLU/Swish': F.silu(x),\n",
    "    'Softplus': F.softplus(x)\n",
    "}\n",
    "\n",
    "# Print some example values\n",
    "test_vals = torch.tensor([-2., -1., 0., 1., 2.])\n",
    "print(\"=== Activation Function Values ===\")\n",
    "print(f\"Input: {test_vals}\")\n",
    "for name in ['ReLU', 'Sigmoid', 'Tanh', 'GELU']:\n",
    "    if name == 'ReLU':\n",
    "        result = torch.relu(test_vals)\n",
    "    elif name == 'Sigmoid':\n",
    "        result = torch.sigmoid(test_vals)\n",
    "    elif name == 'Tanh':\n",
    "        result = torch.tanh(test_vals)\n",
    "    elif name == 'GELU':\n",
    "        result = F.gelu(test_vals)\n",
    "    print(f\"{name:8}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "activation_visualization"
   },
   "source": [
    "## 4. Activation Function Visualization\n",
    "\n",
    "Let's visualize different activation functions to understand their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_activations"
   },
   "outputs": [],
   "source": [
    "# Plot activation functions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "\n",
    "activations = [\n",
    "    ('ReLU', torch.relu(x)),\n",
    "    ('Sigmoid', torch.sigmoid(x)),\n",
    "    ('Tanh', torch.tanh(x)),\n",
    "    ('LeakyReLU', F.leaky_relu(x, 0.1)),\n",
    "    ('ELU', F.elu(x)),\n",
    "    ('GELU', F.gelu(x)),\n",
    "    ('SiLU/Swish', F.silu(x)),\n",
    "    ('Softplus', F.softplus(x))\n",
    "]\n",
    "\n",
    "for i, (name, y) in enumerate(activations):\n",
    "    axes[i].plot(x.numpy(), y.numpy(), linewidth=2, color='blue')\n",
    "    axes[i].set_title(f'{name}', fontsize=12, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[i].axvline(x=0, color='k', linewidth=0.5)\n",
    "    axes[i].set_xlabel('x')\n",
    "    axes[i].set_ylabel(f'{name}(x)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print characteristics\n",
    "print(\"\\n=== Activation Function Characteristics ===\")\n",
    "print(\"ReLU: Simple, fast, but can cause dead neurons (gradient = 0 for x < 0)\")\n",
    "print(\"Sigmoid: Smooth, bounded [0,1], but suffers from vanishing gradients\")\n",
    "print(\"Tanh: Zero-centered, bounded [-1,1], better than sigmoid\")\n",
    "print(\"LeakyReLU: Prevents dead neurons with small negative slope\")\n",
    "print(\"ELU: Smooth, can produce negative outputs\")\n",
    "print(\"GELU: Smooth approximation to ReLU, used in transformers\")\n",
    "print(\"SiLU/Swish: Smooth, self-gated, x * sigmoid(x)\")\n",
    "print(\"Softplus: Smooth approximation to ReLU, log(1 + exp(x))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "derivatives"
   },
   "source": [
    "## 5. Derivatives and Gradients\n",
    "\n",
    "Understanding gradients is crucial for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gradient_computation"
   },
   "outputs": [],
   "source": [
    "# Compute gradients of activation functions\n",
    "x = torch.linspace(-3, 3, 100, requires_grad=True)\n",
    "\n",
    "# Function to compute derivative\n",
    "def compute_derivative(func, x_vals):\n",
    "    y = func(x_vals)\n",
    "    # Compute gradient\n",
    "    grad_outputs = torch.ones_like(y)\n",
    "    grads = torch.autograd.grad(outputs=y, inputs=x_vals,\n",
    "                               grad_outputs=grad_outputs,\n",
    "                               create_graph=True, retain_graph=True)[0]\n",
    "    return grads\n",
    "\n",
    "# Plot functions and their derivatives\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "functions = [\n",
    "    ('ReLU', torch.relu),\n",
    "    ('Sigmoid', torch.sigmoid),\n",
    "    ('Tanh', torch.tanh),\n",
    "    ('GELU', F.gelu)\n",
    "]\n",
    "\n",
    "for i, (name, func) in enumerate(functions):\n",
    "    row, col = i // 2, i % 2\n",
    "    \n",
    "    # Reset gradients\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()\n",
    "    \n",
    "    # Function values\n",
    "    y = func(x)\n",
    "    \n",
    "    # Derivatives\n",
    "    try:\n",
    "        dy_dx = compute_derivative(func, x)\n",
    "        \n",
    "        # Plot function and derivative\n",
    "        axes[row, col].plot(x.detach().numpy(), y.detach().numpy(), \n",
    "                           label=f'{name}', linewidth=2)\n",
    "        axes[row, col].plot(x.detach().numpy(), dy_dx.detach().numpy(), \n",
    "                           label=f\"{name}'\", linewidth=2, linestyle='--')\n",
    "    except:\n",
    "        # For functions that don't support autograd everywhere\n",
    "        axes[row, col].plot(x.detach().numpy(), y.detach().numpy(), \n",
    "                           label=f'{name}', linewidth=2)\n",
    "    \n",
    "    axes[row, col].set_title(f'{name} and its Derivative')\n",
    "    axes[row, col].legend()\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    axes[row, col].axhline(y=0, color='k', linewidth=0.5)\n",
    "    axes[row, col].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "softmax"
   },
   "source": [
    "## 6. Softmax Function\n",
    "\n",
    "The softmax function is crucial for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "softmax_examples"
   },
   "outputs": [],
   "source": [
    "# Softmax examples\n",
    "print(\"=== Softmax Function ===\")\n",
    "\n",
    "# Example 1: Basic softmax\n",
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "probs = F.softmax(logits, dim=0)\n",
    "print(f\"Logits: {logits}\")\n",
    "print(f\"Softmax: {probs}\")\n",
    "print(f\"Sum: {probs.sum()}\")\n",
    "\n",
    "# Example 2: Batch softmax\n",
    "batch_logits = torch.tensor([[2.0, 1.0, 0.1],\n",
    "                            [0.5, 2.5, 1.0],\n",
    "                            [1.0, 1.0, 1.0]])\n",
    "batch_probs = F.softmax(batch_logits, dim=1)\n",
    "print(f\"\\nBatch logits:\\n{batch_logits}\")\n",
    "print(f\"\\nBatch softmax:\\n{batch_probs}\")\n",
    "print(f\"Row sums: {batch_probs.sum(dim=1)}\")\n",
    "\n",
    "# Temperature scaling\n",
    "print(f\"\\n=== Temperature Scaling ===\")\n",
    "temperatures = [0.5, 1.0, 2.0, 5.0]\n",
    "for temp in temperatures:\n",
    "    temp_probs = F.softmax(logits / temp, dim=0)\n",
    "    print(f\"Temperature {temp}: {temp_probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stability"
   },
   "source": [
    "## 7. Numerical Stability\n",
    "\n",
    "Understanding numerical stability in mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stability_demo"
   },
   "outputs": [],
   "source": [
    "print(\"=== Numerical Stability ===\")\n",
    "\n",
    "# Large logits can cause numerical instability\n",
    "large_logits = torch.tensor([1000., 999., 998.])\n",
    "\n",
    "# Naive softmax (might overflow)\n",
    "print(f\"Large logits: {large_logits}\")\n",
    "print(f\"exp(logits): {torch.exp(large_logits)}\")\n",
    "\n",
    "# Stable softmax (PyTorch handles this automatically)\n",
    "stable_probs = F.softmax(large_logits, dim=0)\n",
    "print(f\"Stable softmax: {stable_probs}\")\n",
    "\n",
    "# Log-softmax for numerical stability\n",
    "log_probs = F.log_softmax(large_logits, dim=0)\n",
    "print(f\"Log-softmax: {log_probs}\")\n",
    "\n",
    "# Verify: exp(log_softmax) = softmax\n",
    "reconstructed_probs = torch.exp(log_probs)\n",
    "print(f\"Reconstructed: {reconstructed_probs}\")\n",
    "print(f\"Match: {torch.allclose(stable_probs, reconstructed_probs)}\")\n",
    "\n",
    "# LogSumExp trick\n",
    "print(f\"\\n=== LogSumExp Trick ===\")\n",
    "def manual_logsumexp(x):\n",
    "    max_x = torch.max(x)\n",
    "    return max_x + torch.log(torch.sum(torch.exp(x - max_x)))\n",
    "\n",
    "manual_lse = manual_logsumexp(large_logits)\n",
    "pytorch_lse = torch.logsumexp(large_logits, dim=0)\n",
    "print(f\"Manual LogSumExp: {manual_lse}\")\n",
    "print(f\"PyTorch LogSumExp: {pytorch_lse}\")\n",
    "print(f\"Match: {torch.allclose(manual_lse, pytorch_lse)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercises"
   },
   "source": [
    "## 8. Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise1"
   },
   "outputs": [],
   "source": [
    "# Exercise 1: Implement a custom activation function\n",
    "print(\"Exercise 1: Custom Swish activation function\")\n",
    "\n",
    "def custom_swish(x, beta=1.0):\n",
    "    \"\"\"Custom implementation of Swish: x * sigmoid(beta * x)\"\"\"\n",
    "    return x * torch.sigmoid(beta * x)\n",
    "\n",
    "# Test implementation\n",
    "x = torch.linspace(-3, 3, 7)\n",
    "custom_result = custom_swish(x)\n",
    "pytorch_result = F.silu(x)  # SiLU is Swish with beta=1\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Custom Swish: {custom_result}\")\n",
    "print(f\"PyTorch SiLU: {pytorch_result}\")\n",
    "print(f\"Match: {torch.allclose(custom_result, pytorch_result)}\")\n",
    "\n",
    "# Try different beta values\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_plot = torch.linspace(-5, 5, 100)\n",
    "for beta in [0.5, 1.0, 1.5, 2.0]:\n",
    "    y = custom_swish(x_plot, beta)\n",
    "    plt.plot(x_plot.numpy(), y.numpy(), label=f'β={beta}', linewidth=2)\n",
    "\n",
    "plt.title('Swish Activation with Different β Values')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Swish(x, β)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise2"
   },
   "outputs": [],
   "source": [
    "# Exercise 2: Activation function comparison for gradient flow\n",
    "print(\"Exercise 2: Gradient flow comparison\")\n",
    "\n",
    "# Create input with gradient tracking\n",
    "x = torch.linspace(-5, 5, 100, requires_grad=True)\n",
    "\n",
    "# Compare gradient magnitudes for different activations\n",
    "activations = {\n",
    "    'ReLU': torch.relu,\n",
    "    'Sigmoid': torch.sigmoid,\n",
    "    'Tanh': torch.tanh,\n",
    "    'GELU': F.gelu\n",
    "}\n",
    "\n",
    "gradient_stats = {}\n",
    "\n",
    "for name, func in activations.items():\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()\n",
    "    \n",
    "    y = func(x)\n",
    "    \n",
    "    # Compute gradients\n",
    "    grad_outputs = torch.ones_like(y)\n",
    "    try:\n",
    "        grads = torch.autograd.grad(outputs=y, inputs=x,\n",
    "                                   grad_outputs=grad_outputs,\n",
    "                                   retain_graph=True)[0]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        gradient_stats[name] = {\n",
    "            'mean_abs_grad': torch.mean(torch.abs(grads)).item(),\n",
    "            'max_grad': torch.max(torch.abs(grads)).item(),\n",
    "            'zero_grads': torch.sum(grads == 0).item()\n",
    "        }\n",
    "    except:\n",
    "        gradient_stats[name] = {'error': 'Could not compute gradients'}\n",
    "\n",
    "# Display results\n",
    "print(\"\\nGradient Statistics:\")\n",
    "for name, stats in gradient_stats.items():\n",
    "    if 'error' not in stats:\n",
    "        print(f\"{name:8} - Mean |grad|: {stats['mean_abs_grad']:.4f}, \"\n",
    "              f\"Max |grad|: {stats['max_grad']:.4f}, \"\n",
    "              f\"Zero grads: {stats['zero_grads']}\")\n",
    "    else:\n",
    "        print(f\"{name:8} - {stats['error']}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Higher mean absolute gradients suggest better gradient flow\")\n",
    "print(\"- Zero gradients indicate potential dead neurons (especially in ReLU)\")\n",
    "print(\"- Sigmoid/Tanh may have vanishing gradients in saturation regions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored:\n",
    "\n",
    "1. **Advanced Mathematical Operations**: Trigonometric, exponential, and power functions\n",
    "2. **Linear Algebra**: Matrix operations, determinants, eigenvalues, and more\n",
    "3. **Activation Functions**: Common activation functions used in neural networks\n",
    "4. **Visualization**: Plotting activation functions and their derivatives\n",
    "5. **Gradients**: Understanding gradient computation for different functions\n",
    "6. **Softmax**: Multi-class classification probability distributions\n",
    "7. **Numerical Stability**: Techniques for stable computation with large numbers\n",
    "8. **Custom Functions**: Implementing your own activation functions\n",
    "\n",
    "### Key Takeaways\n",
    "- Different activation functions have different properties for gradient flow\n",
    "- ReLU is fast but can cause dead neurons\n",
    "- Sigmoid and Tanh can suffer from vanishing gradients\n",
    "- Modern activations like GELU and Swish offer smooth alternatives\n",
    "- Numerical stability is crucial for reliable computations\n",
    "\n",
    "### Next Steps\n",
    "- Experiment with different activation functions in neural networks\n",
    "- Try implementing other activation functions (Mish, PReLU, etc.)\n",
    "- Understand how activation choice affects training dynamics\n",
    "- Move on to the next notebook: Automatic Differentiation"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM...",
   "collapsed_sections": [],
   "name": "02_mathematical_operations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}