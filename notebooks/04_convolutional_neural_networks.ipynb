{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/your-username/pytorch-for-deeplearning/blob/main/notebooks/04_convolutional_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Chapter 4: Convolutional Neural Networks\n",
    "\n",
    "This notebook explores CNNs - the foundation of modern computer vision.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand convolutional layers and their parameters\n",
    "- Learn about pooling operations\n",
    "- Build complete CNN architectures\n",
    "- Train a CNN on image classification\n",
    "- Visualize learned features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install and import necessary libraries\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    !pip install torch torchvision torchaudio\n",
    "    import torch\n",
    "    print(f\"PyTorch installed. Version: {torch.__version__}\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device and random seed\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conv_basics"
   },
   "source": [
    "## 1. Convolution Operation Basics\n",
    "\n",
    "Understanding how convolution works at the fundamental level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "conv_demo"
   },
   "outputs": [],
   "source": [
    "print(\"=== Basic Convolution Operation ===\")\n",
    "\n",
    "# Create a simple 5x5 input\n",
    "input_2d = torch.tensor([\n",
    "    [1, 2, 3, 0, 1],\n",
    "    [0, 1, 2, 3, 1],\n",
    "    [1, 0, 1, 2, 3],\n",
    "    [2, 1, 0, 1, 2],\n",
    "    [1, 2, 1, 0, 1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"Input (5x5):\")\n",
    "print(input_2d)\n",
    "\n",
    "# Add batch and channel dimensions: (batch_size, channels, height, width)\n",
    "input_4d = input_2d.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 5, 5)\n",
    "print(f\"\\nInput shape: {input_4d.shape}\")\n",
    "\n",
    "# Create different kernels\n",
    "kernels = {\n",
    "    'identity': torch.tensor([[0, 0, 0], [0, 1, 0], [0, 0, 0]]),\n",
    "    'edge_horizontal': torch.tensor([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]),\n",
    "    'edge_vertical': torch.tensor([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]),\n",
    "    'sharpen': torch.tensor([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "}\n",
    "\n",
    "# Apply convolutions\n",
    "results = {}\n",
    "for name, kernel in kernels.items():\n",
    "    # Add dimensions to kernel: (out_channels, in_channels, height, width)\n",
    "    kernel_4d = kernel.unsqueeze(0).unsqueeze(0).float()\n",
    "    \n",
    "    # Apply convolution\n",
    "    output = F.conv2d(input_4d, kernel_4d, padding=0)\n",
    "    results[name] = output.squeeze()  # Remove batch and channel dims\n",
    "    \n",
    "    print(f\"\\n{name.title()} kernel:\")\n",
    "    print(kernel)\n",
    "    print(f\"Output (3x3):\")\n",
    "    print(results[name])\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Original input\n",
    "im0 = axes[0].imshow(input_2d.numpy(), cmap='viridis')\n",
    "axes[0].set_title('Original Input', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "# Kernel outputs\n",
    "for i, (name, output) in enumerate(results.items(), 1):\n",
    "    im = axes[i].imshow(output.numpy(), cmap='viridis')\n",
    "    axes[i].set_title(f'{name.title()} Filter', fontsize=14, fontweight='bold')\n",
    "    plt.colorbar(im, ax=axes[i])\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[5].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conv_layers"
   },
   "source": [
    "## 2. Convolutional Layers\n",
    "\n",
    "Building and understanding Conv2d layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "conv_layer_demo"
   },
   "outputs": [],
   "source": [
    "print(\"=== Convolutional Layers ===\")\n",
    "\n",
    "# Create different convolutional layers\n",
    "conv_configs = [\n",
    "    {'in_channels': 3, 'out_channels': 16, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'in_channels': 3, 'out_channels': 16, 'kernel_size': 5, 'stride': 1, 'padding': 2},\n",
    "    {'in_channels': 3, 'out_channels': 16, 'kernel_size': 3, 'stride': 2, 'padding': 1},\n",
    "    {'in_channels': 3, 'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 0},\n",
    "]\n",
    "\n",
    "# Create sample input (batch_size=4, channels=3, height=32, width=32)\n",
    "sample_input = torch.randn(4, 3, 32, 32)\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "\n",
    "for i, config in enumerate(conv_configs):\n",
    "    conv = nn.Conv2d(**config)\n",
    "    output = conv(sample_input)\n",
    "    \n",
    "    print(f\"\\nConv {i+1} - {config}\")\n",
    "    print(f\"  Parameter count: {sum(p.numel() for p in conv.parameters()):,}\")\n",
    "    print(f\"  Weight shape: {conv.weight.shape}\")\n",
    "    print(f\"  Bias shape: {conv.bias.shape if conv.bias is not None else 'None'}\")\n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    \n",
    "    # Calculate expected output size\n",
    "    h_out = (32 + 2*config['padding'] - config['kernel_size']) // config['stride'] + 1\n",
    "    w_out = (32 + 2*config['padding'] - config['kernel_size']) // config['stride'] + 1\n",
    "    print(f\"  Expected spatial: {h_out}Ã—{w_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pooling"
   },
   "source": [
    "## 3. Pooling Operations\n",
    "\n",
    "Understanding max pooling, average pooling, and adaptive pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pooling_demo"
   },
   "outputs": [],
   "source": [
    "print(\"=== Pooling Operations ===\")\n",
    "\n",
    "# Create sample feature map\n",
    "feature_map = torch.randn(1, 64, 16, 16)  # (batch, channels, height, width)\n",
    "print(f\"Input feature map shape: {feature_map.shape}\")\n",
    "\n",
    "# Different pooling operations\n",
    "pooling_ops = {\n",
    "    'MaxPool2d(2, 2)': nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    'MaxPool2d(3, 2)': nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "    'AvgPool2d(2, 2)': nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    'AdaptiveMaxPool2d(8, 8)': nn.AdaptiveMaxPool2d((8, 8)),\n",
    "    'AdaptiveAvgPool2d(1, 1)': nn.AdaptiveAvgPool2d((1, 1)),  # Global pooling\n",
    "}\n",
    "\n",
    "for name, pool_op in pooling_ops.items():\n",
    "    output = pool_op(feature_map)\n",
    "    reduction = feature_map.numel() / output.numel()\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    print(f\"  Size reduction: {reduction:.1f}x\")\n",
    "\n",
    "# Demonstrate pooling on actual values\n",
    "print(\"\\n=== Pooling Example on 4x4 input ===\")\n",
    "small_input = torch.tensor([\n",
    "    [[[ 1,  2,  3,  4],\n",
    "      [ 5,  6,  7,  8],\n",
    "      [ 9, 10, 11, 12],\n",
    "      [13, 14, 15, 16]]]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"Input (1x1x4x4):\")\n",
    "print(small_input.squeeze().int())\n",
    "\n",
    "# Max pooling 2x2\n",
    "max_pool = nn.MaxPool2d(2, 2)\n",
    "max_output = max_pool(small_input)\n",
    "print(f\"\\nMax pool 2x2 output (1x1x2x2):\")\n",
    "print(max_output.squeeze().int())\n",
    "\n",
    "# Average pooling 2x2\n",
    "avg_pool = nn.AvgPool2d(2, 2)\n",
    "avg_output = avg_pool(small_input)\n",
    "print(f\"\\nAvg pool 2x2 output (1x1x2x2):\")\n",
    "print(avg_output.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnn_architecture"
   },
   "source": [
    "## 4. CNN Architecture Design\n",
    "\n",
    "Building complete CNN models from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "simple_cnn"
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"A simple CNN for CIFAR-10 classification\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 32x32 -> 16x16\n",
    "        )\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 16x16 -> 8x8\n",
    "        )\n",
    "        \n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 8x8 -> 4x4\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create and analyze the model\n",
    "model = SimpleCNN(num_classes=10).to(device)\n",
    "print(\"=== Simple CNN Architecture ===\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "sample_input = torch.randn(4, 3, 32, 32).to(device)  # CIFAR-10 input size\n",
    "with torch.no_grad():\n",
    "    output = model(sample_input)\n",
    "    \n",
    "print(f\"\\nSample input shape: {sample_input.shape}\")\n",
    "print(f\"Model output shape: {output.shape}\")\n",
    "print(f\"Output (logits): {output[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intermediate_features"
   },
   "source": [
    "## 5. Visualizing Intermediate Features\n",
    "\n",
    "Understanding what CNNs learn at different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature_visualization"
   },
   "outputs": [],
   "source": [
    "# Create a hook to capture intermediate activations\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, model, layer_name):\n",
    "        self.features = None\n",
    "        self.hook = self._get_layer(model, layer_name).register_forward_hook(self.hook_fn)\n",
    "    \n",
    "    def _get_layer(self, model, layer_name):\n",
    "        \"\"\"Get layer by name\"\"\"\n",
    "        for name, layer in model.named_modules():\n",
    "            if name == layer_name:\n",
    "                return layer\n",
    "        raise ValueError(f\"Layer {layer_name} not found\")\n",
    "    \n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features = output.detach()\n",
    "    \n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "# Create sample data (a simple synthetic image)\n",
    "def create_sample_image():\n",
    "    \"\"\"Create a simple synthetic image with patterns\"\"\"\n",
    "    img = torch.zeros(1, 3, 32, 32)\n",
    "    \n",
    "    # Add some patterns\n",
    "    # Vertical lines in red channel\n",
    "    img[0, 0, :, [5, 15, 25]] = 1.0\n",
    "    \n",
    "    # Horizontal lines in green channel\n",
    "    img[0, 1, [8, 16, 24], :] = 1.0\n",
    "    \n",
    "    # Diagonal pattern in blue channel\n",
    "    for i in range(min(32, 32)):\n",
    "        img[0, 2, i, i] = 1.0\n",
    "        if i < 31:\n",
    "            img[0, 2, i, i+1] = 0.5\n",
    "            img[0, 2, i+1, i] = 0.5\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Create sample input\n",
    "sample_img = create_sample_image().to(device)\n",
    "\n",
    "print(\"=== Feature Visualization ===\")\n",
    "print(f\"Sample image shape: {sample_img.shape}\")\n",
    "\n",
    "# Extract features from different layers\n",
    "layer_names = ['conv_block1.0', 'conv_block2.0', 'conv_block3.0']  # Conv layers\n",
    "extractors = []\n",
    "features = {}\n",
    "\n",
    "try:\n",
    "    # Set up feature extractors\n",
    "    for layer_name in layer_names:\n",
    "        extractor = FeatureExtractor(model, layer_name)\n",
    "        extractors.append(extractor)\n",
    "    \n",
    "    # Forward pass to extract features\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(sample_img)\n",
    "    \n",
    "    # Collect features\n",
    "    for i, extractor in enumerate(extractors):\n",
    "        features[layer_names[i]] = extractor.features\n",
    "        print(f\"Features from {layer_names[i]}: {extractor.features.shape}\")\n",
    "    \n",
    "    # Visualize original image and features\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    # Original image (convert to displayable format)\n",
    "    img_display = sample_img[0].permute(1, 2, 0).cpu().numpy()\n",
    "    axes[0, 0].imshow(img_display)\n",
    "    axes[0, 0].set_title('Original Image', fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Show first few feature maps from each layer\n",
    "    for i, (layer_name, feat) in enumerate(features.items()):\n",
    "        if i < 3:  # Show first 3 layers\n",
    "            # Show first feature map\n",
    "            feature_map = feat[0, 0].cpu().numpy()  # First batch, first channel\n",
    "            axes[0, i+1].imshow(feature_map, cmap='viridis')\n",
    "            axes[0, i+1].set_title(f'{layer_name}\\n(1st feature map)', fontweight='bold')\n",
    "            axes[0, i+1].axis('off')\n",
    "            \n",
    "            # Show another feature map if available\n",
    "            if feat.shape[1] > 1:\n",
    "                feature_map2 = feat[0, min(1, feat.shape[1]-1)].cpu().numpy()\n",
    "                axes[1, i+1].imshow(feature_map2, cmap='viridis')\n",
    "                axes[1, i+1].set_title(f'{layer_name}\\n(2nd feature map)', fontweight='bold')\n",
    "                axes[1, i+1].axis('off')\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    axes[1, 0].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\nfinally:\n",
    "    # Clean up hooks\n",
    "    for extractor in extractors:\n",
    "        extractor.close()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Early layers detect simple features (edges, patterns)\")\n",
    "print(\"- Deeper layers combine simpler features into complex patterns\")\n",
    "print(\"- Feature maps become smaller but more numerous with depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_example"
   },
   "source": [
    "## 6. Training a CNN\n",
    "\n",
    "Complete training example on CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cifar10_training"
   },
   "outputs": [],
   "source": [
    "# Download CIFAR-10 dataset\n",
    "print(\"=== Downloading CIFAR-10 Dataset ===\")\n",
    "\n",
    "# Data transforms\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Download datasets\n",
    "try:\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                          download=True, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                         download=True, transform=transform_test)\n",
    "    \n",
    "    # Create data loaders\n",
    "    trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"Training samples: {len(trainset)}\")\n",
    "    print(f\"Test samples: {len(testset)}\")\n",
    "    print(f\"Number of classes: {len(trainset.classes)}\")\n",
    "    print(f\"Classes: {trainset.classes}\")\n",
    "    \n",
    "    # Show sample images\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    # Denormalize for display\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    images_denorm = images * std + mean\n",
    "    images_denorm = torch.clamp(images_denorm, 0, 1)\n",
    "    \n",
    "    # Plot sample images\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "    for i in range(16):\n",
    "        row, col = i // 8, i % 8\n",
    "        img = images_denorm[i].permute(1, 2, 0)\n",
    "        axes[row, col].imshow(img)\n",
    "        axes[row, col].set_title(f'{trainset.classes[labels[i]]}')\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\nexcept Exception as e:\n",
    "    print(f\"Could not download CIFAR-10: {e}\")\n",
    "    print(\"Creating synthetic data for demonstration...\")\n",
    "    \n",
    "    # Create synthetic data if CIFAR-10 download fails\n",
    "    train_data = torch.randn(1000, 3, 32, 32)\n",
    "    train_labels = torch.randint(0, 10, (1000,))\n",
    "    test_data = torch.randn(200, 3, 32, 32)\n",
    "    test_labels = torch.randint(0, 10, (200,))\n",
    "    \n",
    "    trainloader = DataLoader(TensorDataset(train_data, train_labels), \n",
    "                           batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(TensorDataset(test_data, test_labels), \n",
    "                          batch_size=32, shuffle=False)\n",
    "    \n",
    "    print(\"Using synthetic data for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_cnn"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, trainloader, testloader, num_epochs=5):\n",
    "    \"\"\"Train the CNN model\"\"\"\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Print progress\n",
    "            if i % 200 == 199:\n",
    "                print(f'  Batch [{i+1:4d}] Loss: {running_loss/200:.4f}')\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * correct_test / total_test\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] '\n",
    "              f'Train Acc: {train_acc:.2f}% '\n",
    "              f'Test Acc: {test_acc:.2f}% '\n",
    "              f'LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "    \n",
    "    return train_accs, test_accs\n",
    "\n",
    "# Create and train model\n",
    "model = SimpleCNN(num_classes=10).to(device)\n",
    "train_accs, test_accs = train_model(model, trainloader, testloader, num_epochs=3)\n",
    "\n",
    "print(\"\\n=== Training Complete ===\")\n",
    "print(f\"Final train accuracy: {train_accs[-1]:.2f}%\")\n",
    "print(f\"Final test accuracy: {test_accs[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercises"
   },
   "source": [
    "## 7. Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise1"
   },
   "outputs": [],
   "source": [
    "# Exercise 1: Design a deeper CNN\n",
    "print(\"Exercise 1: Deeper CNN Architecture\")\n",
    "\n",
    "class DeeperCNN(nn.Module):\n",
    "    \"\"\"A deeper CNN with residual-like connections\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DeeperCNN, self).__init__()\n",
    "        \n",
    "        # Initial conv\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Conv blocks with increasing channels\n",
    "        self.conv2 = self._make_layer(64, 64, 2)\n",
    "        self.conv3 = self._make_layer(64, 128, 2, stride=2)  # Downsample\n",
    "        self.conv4 = self._make_layer(128, 256, 2, stride=2)  # Downsample\n",
    "        self.conv5 = self._make_layer(256, 512, 2, stride=2)  # Downsample\n",
    "        \n",
    "        # Global average pooling + classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride=1):\n",
    "        layers = []\n",
    "        # First block (might downsample)\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Remaining blocks\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(nn.Conv2d(out_channels, out_channels, 3, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Analyze the deeper model\n",
    "deeper_model = DeeperCNN().to(device)\n",
    "print(f\"Deeper CNN parameters: {sum(p.numel() for p in deeper_model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(2, 3, 32, 32).to(device)\n",
    "    output = deeper_model(test_input)\n",
    "    print(f\"Input shape: {test_input.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "simple_params = sum(p.numel() for p in model.parameters())\n",
    "deeper_params = sum(p.numel() for p in deeper_model.parameters())\n",
    "print(f\"Simple CNN:  {simple_params:,} parameters\")\n",
    "print(f\"Deeper CNN:  {deeper_params:,} parameters\")\n",
    "print(f\"Ratio: {deeper_params/simple_params:.1f}x more parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise2"
   },
   "outputs": [],
   "source": [
    "# Exercise 2: Kernel visualization\n",
    "print(\"Exercise 2: Learned Kernel Visualization\")\n",
    "\n",
    "def visualize_kernels(model, layer_name, num_kernels=8):\n",
    "    \"\"\"Visualize learned kernels from a conv layer\"\"\"\n",
    "    \n",
    "    # Get the layer\n",
    "    layer = None\n",
    "    for name, module in model.named_modules():\n",
    "        if name == layer_name and isinstance(module, nn.Conv2d):\n",
    "            layer = module\n",
    "            break\n",
    "    \n",
    "    if layer is None:\n",
    "        print(f\"Layer {layer_name} not found or not a Conv2d layer\")\n",
    "        return\n",
    "    \n",
    "    # Get kernels\n",
    "    kernels = layer.weight.data.cpu()  # Shape: (out_channels, in_channels, H, W)\n",
    "    print(f\"Kernel shape: {kernels.shape}\")\n",
    "    \n",
    "    # Normalize kernels for visualization\n",
    "    kernels_norm = (kernels - kernels.min()) / (kernels.max() - kernels.min())\n",
    "    \n",
    "    # Plot first few kernels\n",
    "    num_show = min(num_kernels, kernels.shape[0])\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(num_show):\n",
    "        kernel = kernels_norm[i]\n",
    "        \n",
    "        if kernel.shape[0] == 3:  # RGB channels\n",
    "            # Show as RGB image\n",
    "            kernel_rgb = kernel.permute(1, 2, 0)\n",
    "            axes[i].imshow(kernel_rgb)\n",
    "        else:\n",
    "            # Show first channel as grayscale\n",
    "            axes[i].imshow(kernel[0], cmap='gray')\n",
    "        \n",
    "        axes[i].set_title(f'Kernel {i+1}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize kernels from the first layer\n",
    "print(\"First layer kernels (should detect edges, colors, etc.):\")\n",
    "try:\n",
    "    visualize_kernels(model, 'conv_block1.0', num_kernels=8)\nexcept Exception as e:\n",
    "    print(f\"Could not visualize kernels: {e}\")\n",
    "    print(\"This is normal - the model needs to be trained to learn meaningful kernels\")\n",
    "\n",
    "# Show kernel statistics\n",
    "print(\"\\nKernel Statistics:\")\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        weights = module.weight.data\n",
    "        print(f\"{name:15} - Shape: {tuple(weights.shape)}, \"\n",
    "              f\"Mean: {weights.mean().item():.4f}, \"\n",
    "              f\"Std: {weights.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. **Convolution Basics**: Understanding how convolution operations work\n",
    "2. **Conv2d Layers**: Parameters, input/output shapes, and configurations\n",
    "3. **Pooling Operations**: Max pooling, average pooling, and adaptive pooling\n",
    "4. **CNN Architecture**: Building complete convolutional neural networks\n",
    "5. **Feature Visualization**: Understanding what CNNs learn at different layers\n",
    "6. **Training Example**: Complete training pipeline on CIFAR-10\n",
    "7. **Advanced Architectures**: Deeper networks and kernel visualization\n",
    "\n",
    "### Key Concepts\n",
    "- **Translation Invariance**: CNNs detect features regardless of position\n",
    "- **Parameter Sharing**: Same kernel applied across spatial dimensions\n",
    "- **Hierarchical Features**: Simple to complex feature detection\n",
    "- **Spatial Reduction**: Pooling reduces spatial dimensions while preserving information\n",
    "\n",
    "### CNN Design Principles\n",
    "1. **Early Layers**: Detect low-level features (edges, textures)\n",
    "2. **Middle Layers**: Combine features into patterns and shapes\n",
    "3. **Deep Layers**: High-level semantic features\n",
    "4. **Pooling**: Gradually reduce spatial dimensions\n",
    "5. **Channels**: Increase feature maps as you go deeper\n",
    "\n",
    "### Best Practices\n",
    "- Use batch normalization for stable training\n",
    "- Apply data augmentation to prevent overfitting\n",
    "- Use appropriate pooling strategies\n",
    "- Consider residual connections for very deep networks\n",
    "- Monitor both training and validation accuracy\n",
    "\n",
    "### Next Steps\n",
    "- Experiment with different CNN architectures (ResNet, DenseNet, etc.)\n",
    "- Try transfer learning with pre-trained models\n",
    "- Explore advanced techniques like attention mechanisms\n",
    "- Apply CNNs to different domains (medical imaging, satellite imagery, etc.)\n",
    "- Move on to the next notebook: Advanced Neural Network Architectures"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM...",
   "collapsed_sections": [],
   "name": "04_convolutional_neural_networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}