{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adiel2012/pythorch-for-deeplearning/blob/main/notebooks/01_fundamental_tensor_operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Fundamental Tensor Operations - Deep Dive\n",
    "\n",
    "This comprehensive notebook covers the fundamentals of PyTorch tensors with advanced techniques, performance optimization, and real-world applications.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master advanced tensor creation methods and initialization techniques\n",
    "- Understand tensor properties, memory layout, and device management\n",
    "- Perform complex tensor manipulation and reshaping operations\n",
    "- Apply advanced indexing, slicing, and broadcasting techniques\n",
    "- Optimize performance with efficient memory access patterns\n",
    "- Implement real-world examples from modern deep learning architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch if not already available\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    !pip install torch torchvision torchaudio\n",
    "    import torch\n",
    "    print(f\"PyTorch installed. Version: {torch.__version__}\")\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set device and print comprehensive device information\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n=== Device Information ===\")\n",
    "print(f\"Primary device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"  GPU {i}: {props.name}\")\n",
    "        print(f\"    Memory: {props.total_memory / 1e9:.1f} GB\")\n",
    "        print(f\"    Compute capability: {props.major}.{props.minor}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(f\"\\nRandom seeds set for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Tensor Creation Methods\n",
    "\n",
    "### 1.1 torch.tensor() - Direct Data Creation with Memory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Advanced torch.tensor() Usage ===\")\n",
    "\n",
    "# Create tensors from different data types with analysis\n",
    "import sys\n",
    "\n",
    "# Basic creation with comprehensive analysis\n",
    "scalar = torch.tensor(3.14159)\n",
    "vector = torch.tensor([1, 2, 3, 4, 5])\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "\n",
    "tensors = [scalar, vector, matrix, tensor_3d]\n",
    "names = ['Scalar', 'Vector', 'Matrix', '3D Tensor']\n",
    "\n",
    "for name, tensor in zip(names, tensors):\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Value: {tensor}\")\n",
    "    print(f\"  Shape: {tensor.shape}\")\n",
    "    print(f\"  Dimensions: {tensor.ndim}\")\n",
    "    print(f\"  Elements: {tensor.numel()}\")\n",
    "    print(f\"  Data type: {tensor.dtype}\")\n",
    "    print(f\"  Memory (bytes): {tensor.element_size() * tensor.numel()}\")\n",
    "    print(f\"  Storage size: {tensor.storage().size()}\")\n",
    "    print(f\"  Is contiguous: {tensor.is_contiguous()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type specification and precision analysis\n",
    "print(\"=== Data Type Analysis ===\")\n",
    "\n",
    "# Create tensors with different data types\n",
    "data = [1.5, 2.7, 3.9]\n",
    "dtypes = {\n",
    "    'float16': torch.float16,\n",
    "    'float32': torch.float32, \n",
    "    'float64': torch.float64,\n",
    "    'int32': torch.int32,\n",
    "    'int64': torch.int64,\n",
    "    'bool': torch.bool\n",
    "}\n",
    "\n",
    "original_float64 = torch.tensor(data, dtype=torch.float64)\n",
    "print(f\"Original (float64): {original_float64}\")\n",
    "\n",
    "for name, dtype in dtypes.items():\n",
    "    try:\n",
    "        if dtype == torch.bool:\n",
    "            converted = torch.tensor([1, 0, 1], dtype=dtype)\n",
    "        elif dtype in [torch.int32, torch.int64]:\n",
    "            converted = torch.tensor(data, dtype=dtype)  # Will truncate\n",
    "        else:\n",
    "            converted = original_float64.to(dtype)\n",
    "        \n",
    "        print(f\"{name:8}: {converted}, size: {converted.element_size()} bytes\")\n",
    "        \n",
    "        if dtype in [torch.float16, torch.float32]:\n",
    "            precision_loss = (original_float64 - converted.to(torch.float64)).abs().max()\n",
    "            print(f\"         Precision loss: {precision_loss:.2e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name:8}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory sharing analysis: torch.tensor vs torch.from_numpy\n",
    "print(\"=== Memory Sharing Analysis ===\")\n",
    "\n",
    "# Create NumPy array\n",
    "np_array = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n",
    "print(f\"NumPy array: {np_array}\")\n",
    "\n",
    "# Method 1: torch.tensor() - Always copies data\n",
    "tensor_copy = torch.tensor(np_array)\n",
    "print(f\"\\nTensor (copy): {tensor_copy}\")\n",
    "print(f\"Shares memory: {np.shares_memory(np_array, tensor_copy.numpy())}\")\n",
    "\n",
    "# Method 2: torch.from_numpy() - Shares memory\n",
    "tensor_shared = torch.from_numpy(np_array)\n",
    "print(f\"\\nTensor (shared): {tensor_shared}\")\n",
    "print(f\"Shares memory: {np.shares_memory(np_array, tensor_shared.numpy())}\")\n",
    "\n",
    "# Demonstrate memory sharing\n",
    "print(\"\\n=== Memory Sharing Demonstration ===\")\n",
    "original_value = np_array[0, 0]\n",
    "np_array[0, 0] = 999\n",
    "\n",
    "print(f\"After NumPy modification:\")\n",
    "print(f\"NumPy array: {np_array}\")\n",
    "print(f\"Copied tensor: {tensor_copy}\")\n",
    "print(f\"Shared tensor: {tensor_shared}\")\n",
    "\n",
    "# Restore original value\n",
    "np_array[0, 0] = original_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 torch.zeros() - Zero Initialization Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Advanced Zero Tensor Creation ===\")\n",
    "\n",
    "# Different ways to create zero tensors\n",
    "zeros_1d = torch.zeros(5)\n",
    "zeros_2d = torch.zeros(3, 4)\n",
    "zeros_3d = torch.zeros(2, 3, 4)\n",
    "zeros_tuple = torch.zeros((2, 3, 4))\n",
    "\n",
    "print(f\"1D zeros: {zeros_1d}\")\n",
    "print(f\"2D zeros shape: {zeros_2d.shape}\")\n",
    "print(f\"3D zeros shape: {zeros_3d.shape}\")\n",
    "\n",
    "# Template-based creation\n",
    "original = torch.randn(3, 4, dtype=torch.float32)\n",
    "same_shape_zeros = torch.zeros_like(original)\n",
    "device_zeros = original.new_zeros(5, 5)\n",
    "\n",
    "print(f\"\\nOriginal shape: {original.shape}, dtype: {original.dtype}\")\n",
    "print(f\"Zeros-like shape: {same_shape_zeros.shape}, dtype: {same_shape_zeros.dtype}\")\n",
    "print(f\"New zeros shape: {device_zeros.shape}, dtype: {device_zeros.dtype}\")\n",
    "\n",
    "# Different data types\n",
    "int_zeros = torch.zeros(3, 3, dtype=torch.int32)\n",
    "bool_zeros = torch.zeros(2, 2, dtype=torch.bool)\n",
    "complex_zeros = torch.zeros(2, 2, dtype=torch.complex64)\n",
    "\n",
    "print(f\"\\nInteger zeros:\\n{int_zeros}\")\n",
    "print(f\"Boolean zeros:\\n{bool_zeros}\")\n",
    "print(f\"Complex zeros:\\n{complex_zeros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Applications: Attention Masks\n",
    "print(\"=== Neural Network Applications ===\")\n",
    "\n",
    "def create_attention_mask(seq_lengths: List[int], max_len: int) -> torch.Tensor:\n",
    "    \"\"\"Create attention mask for variable-length sequences\"\"\"\n",
    "    batch_size = len(seq_lengths)\n",
    "    mask = torch.zeros(batch_size, max_len, dtype=torch.bool)\n",
    "    \n",
    "    for i, length in enumerate(seq_lengths):\n",
    "        mask[i, :length] = True\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def create_causal_mask(seq_len: int) -> torch.Tensor:\n",
    "    \"\"\"Create lower triangular mask for causal attention\"\"\"\n",
    "    mask = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
    "    for i in range(seq_len):\n",
    "        mask[i, :i+1] = True\n",
    "    return mask\n",
    "\n",
    "# Example usage\n",
    "seq_lengths = [5, 8, 3, 6]\n",
    "max_len = 10\n",
    "attention_mask = create_attention_mask(seq_lengths, max_len)\n",
    "\n",
    "print(f\"Sequence lengths: {seq_lengths}\")\n",
    "print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "print(f\"Attention mask (1=valid, 0=padding):\\n{attention_mask.int()}\")\n",
    "\n",
    "# Causal mask for autoregressive models\n",
    "causal_mask = create_causal_mask(5)\n",
    "print(f\"\\nCausal mask (1=can attend, 0=masked):\\n{causal_mask.int()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 torch.randn() - Advanced Random Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Advanced Random Tensor Creation ===\")\n",
    "\n",
    "# Statistical analysis of random tensors\n",
    "large_sample = torch.randn(10000)\n",
    "print(f\"Large sample statistics (n=10000):\")\n",
    "print(f\"Mean: {large_sample.mean():.6f} (expected: ~0.0)\")\n",
    "print(f\"Std: {large_sample.std():.6f} (expected: ~1.0)\")\n",
    "print(f\"Min: {large_sample.min():.4f}\")\n",
    "print(f\"Max: {large_sample.max():.4f}\")\n",
    "\n",
    "# Generator-based seeding for reproducibility\n",
    "g1 = torch.Generator().manual_seed(12345)\n",
    "g2 = torch.Generator().manual_seed(12345)\n",
    "\n",
    "repro1 = torch.randn(5, generator=g1)\n",
    "repro2 = torch.randn(5, generator=g2)\n",
    "\n",
    "print(f\"\\nReproducibility test:\")\n",
    "print(f\"Generator 1: {repro1}\")\n",
    "print(f\"Generator 2: {repro2}\")\n",
    "print(f\"Identical: {torch.equal(repro1, repro2)}\")\n",
    "\n",
    "# Multiple generators for different purposes\n",
    "weight_gen = torch.Generator().manual_seed(1)\n",
    "bias_gen = torch.Generator().manual_seed(2)\n",
    "noise_gen = torch.Generator().manual_seed(3)\n",
    "\n",
    "weights = torch.randn(10, 5, generator=weight_gen)\n",
    "biases = torch.randn(5, generator=bias_gen)\n",
    "noise = torch.randn(32, 10, generator=noise_gen)\n",
    "\n",
    "print(f\"\\nMultiple generator usage:\")\n",
    "print(f\"Weights shape: {weights.shape}\")\n",
    "print(f\"Biases shape: {biases.shape}\")\n",
    "print(f\"Noise shape: {noise.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced initialization techniques for neural networks\n",
    "print(\"=== Neural Network Initialization Techniques ===\")\n",
    "\n",
    "def xavier_init(in_features: int, out_features: int, generator=None) -> torch.Tensor:\n",
    "    \"\"\"Xavier/Glorot initialization for better gradient flow\"\"\"\n",
    "    W = torch.randn(out_features, in_features, generator=generator)\n",
    "    fan_in, fan_out = in_features, out_features\n",
    "    std = (2.0 / (fan_in + fan_out)) ** 0.5\n",
    "    W *= std\n",
    "    return W\n",
    "\n",
    "def kaiming_init(in_features: int, out_features: int, generator=None) -> torch.Tensor:\n",
    "    \"\"\"Kaiming/He initialization for ReLU networks\"\"\"\n",
    "    W = torch.randn(out_features, in_features, generator=generator)\n",
    "    fan_in = in_features\n",
    "    std = (2.0 / fan_in) ** 0.5\n",
    "    W *= std\n",
    "    return W\n",
    "\n",
    "def orthogonal_init(shape: Tuple[int, int], generator=None) -> torch.Tensor:\n",
    "    \"\"\"Orthogonal initialization using QR decomposition\"\"\"\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Orthogonal init only works for 2D tensors\")\n",
    "    \n",
    "    rows, cols = shape\n",
    "    W = torch.randn(rows, cols, generator=generator)\n",
    "    Q, R = torch.linalg.qr(W)\n",
    "    d = torch.diag(R, 0)\n",
    "    Q *= d.sign().unsqueeze(0).expand_as(Q)\n",
    "    return Q\n",
    "\n",
    "# Compare initialization methods\n",
    "layers = [(784, 256), (256, 128), (128, 64), (64, 10)]\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "print(\"Initialization Comparison:\")\n",
    "for i, (in_feat, out_feat) in enumerate(layers):\n",
    "    # Standard randn\n",
    "    std_W = torch.randn(out_feat, in_feat, generator=gen)\n",
    "    xavier_W = xavier_init(in_feat, out_feat, generator=gen)\n",
    "    kaiming_W = kaiming_init(in_feat, out_feat, generator=gen)\n",
    "    \n",
    "    print(f\"\\nLayer {i+1}: ({in_feat} -> {out_feat})\")\n",
    "    print(f\"  Standard - Mean: {std_W.mean():.4f}, Std: {std_W.std():.4f}\")\n",
    "    print(f\"  Xavier   - Mean: {xavier_W.mean():.4f}, Std: {xavier_W.std():.4f}\")\n",
    "    print(f\"  Kaiming  - Mean: {kaiming_W.mean():.4f}, Std: {kaiming_W.std():.4f}\")\n",
    "\n",
    "# Test orthogonal initialization\n",
    "ortho_matrix = orthogonal_init((100, 100))\n",
    "identity_check = torch.mm(ortho_matrix, ortho_matrix.t())\n",
    "orthogonality_error = (identity_check - torch.eye(100)).abs().max()\n",
    "\n",
    "print(f\"\\nOrthogonal Initialization:\")\n",
    "print(f\"Matrix shape: {ortho_matrix.shape}\")\n",
    "print(f\"Orthogonality error: {orthogonality_error:.6f}\")\n",
    "print(f\"Should be identity (first 3x3):\\n{identity_check[:3, :3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 torch.arange() - Sequence Generation and Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Advanced Sequence Generation ===\")\n",
    "\n",
    "# Basic sequence creation with analysis\n",
    "seq1 = torch.arange(5)\n",
    "seq2 = torch.arange(1, 6)\n",
    "seq3 = torch.arange(0, 10, 2)\n",
    "seq4 = torch.arange(10, 0, -1)\n",
    "\n",
    "sequences = [seq1, seq2, seq3, seq4]\n",
    "descriptions = ['arange(5)', 'arange(1,6)', 'arange(0,10,2)', 'arange(10,0,-1)']\n",
    "\n",
    "for desc, seq in zip(descriptions, sequences):\n",
    "    print(f\"{desc:15}: {seq}\")\n",
    "\n",
    "# Floating-point sequences\n",
    "float_seq = torch.arange(0.0, 2.0, 0.3)\n",
    "print(f\"\\nFloat sequence: {float_seq}\")\n",
    "\n",
    "# Data type control\n",
    "long_seq = torch.arange(5, dtype=torch.long)\n",
    "float_seq = torch.arange(5, dtype=torch.float32)\n",
    "print(f\"\\nLong dtype: {long_seq}, dtype: {long_seq.dtype}\")\n",
    "print(f\"Float dtype: {float_seq}, dtype: {float_seq.dtype}\")\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\n=== Performance Comparison ===\")\n",
    "import time\n",
    "\n",
    "# Method 1: torch.arange\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    seq_arange = torch.arange(1000)\n",
    "arange_time = time.time() - start\n",
    "\n",
    "# Method 2: torch.tensor from Python range\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    seq_tensor = torch.tensor(list(range(1000)))\n",
    "tensor_time = time.time() - start\n",
    "\n",
    "print(f\"torch.arange: {arange_time:.4f}s\")\n",
    "print(f\"torch.tensor(list(range)): {tensor_time:.4f}s\")\n",
    "print(f\"Speedup: {tensor_time/arange_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced applications: Position embeddings for Transformers\n",
    "print(\"=== Position Embeddings for Transformers ===\")\n",
    "\n",
    "class PositionalEmbedding:\n",
    "    def __init__(self, max_seq_len: int, embedding_dim: int):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # Learnable position embeddings\n",
    "        self.position_embeddings = torch.randn(max_seq_len, embedding_dim) * 0.02\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        assert seq_len <= self.max_seq_len\n",
    "        \n",
    "        # Create position indices using arange\n",
    "        positions = torch.arange(0, seq_len, dtype=torch.long, device=input_ids.device)\n",
    "        positions = positions.unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # Get position embeddings\n",
    "        pos_embeddings = self.position_embeddings[positions]\n",
    "        return pos_embeddings\n",
    "\n",
    "def create_sinusoidal_embeddings(max_seq_len: int, embedding_dim: int) -> torch.Tensor:\n",
    "    \"\"\"Create fixed sinusoidal position embeddings\"\"\"\n",
    "    position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, embedding_dim, 2, dtype=torch.float) *\n",
    "                        -(math.log(10000.0) / embedding_dim))\n",
    "    \n",
    "    pe = torch.zeros(max_seq_len, embedding_dim)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# Example usage\n",
    "max_len, emb_dim = 128, 64\n",
    "pos_emb = PositionalEmbedding(max_len, emb_dim)\n",
    "\n",
    "# Simulate input batch\n",
    "batch_size, seq_len = 8, 32\n",
    "input_ids = torch.randint(0, 1000, (batch_size, seq_len))\n",
    "\n",
    "# Get position embeddings\n",
    "position_embeds = pos_emb.forward(input_ids)\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Position embeddings shape: {position_embeds.shape}\")\n",
    "\n",
    "# Create sinusoidal embeddings\n",
    "sinusoidal_pe = create_sinusoidal_embeddings(50, 32)\n",
    "print(f\"\\nSinusoidal PE shape: {sinusoidal_pe.shape}\")\n",
    "print(f\"First position embedding: {sinusoidal_pe[0, :8]}\")\n",
    "print(f\"Second position embedding: {sinusoidal_pe[1, :8]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinate generation for computer vision\n",
    "print(\"=== Coordinate Grids for Computer Vision ===\")\n",
    "\n",
    "def create_coordinate_grid(height: int, width: int, device='cpu') -> torch.Tensor:\n",
    "    \"\"\"Create coordinate grid for spatial operations\"\"\"\n",
    "    y_coords = torch.arange(height, dtype=torch.float32, device=device)\n",
    "    x_coords = torch.arange(width, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Create 2D coordinate grids\n",
    "    grid_y, grid_x = torch.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    \n",
    "    # Stack into single tensor: (2, height, width)\n",
    "    coords = torch.stack([grid_x, grid_y], dim=0)\n",
    "    return coords\n",
    "\n",
    "def create_normalized_grid(height: int, width: int, device='cpu') -> torch.Tensor:\n",
    "    \"\"\"Create normalized coordinate grid (-1 to 1)\"\"\"\n",
    "    y_coords = torch.arange(height, dtype=torch.float32, device=device)\n",
    "    x_coords = torch.arange(width, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Normalize to [-1, 1]\n",
    "    y_coords = (y_coords / (height - 1)) * 2 - 1\n",
    "    x_coords = (x_coords / (width - 1)) * 2 - 1\n",
    "    \n",
    "    grid_y, grid_x = torch.meshgrid(y_coords, x_coords, indexing='ij')\n",
    "    return torch.stack([grid_x, grid_y], dim=0)\n",
    "\n",
    "# Example usage\n",
    "coord_grid = create_coordinate_grid(4, 6)\n",
    "norm_grid = create_normalized_grid(4, 6)\n",
    "\n",
    "print(f\"Coordinate grid shape: {coord_grid.shape}\")\n",
    "print(f\"X coordinates:\\n{coord_grid[0]}\")\n",
    "print(f\"Y coordinates:\\n{coord_grid[1]}\")\n",
    "\n",
    "print(f\"\\nNormalized grid X range: [{norm_grid[0].min():.2f}, {norm_grid[0].max():.2f}]\")\n",
    "print(f\"Normalized grid Y range: [{norm_grid[1].min():.2f}, {norm_grid[1].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comprehensive Tensor Properties and Memory Management\n",
    "\n",
    "### 2.1 Shape Analysis and Broadcasting Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Comprehensive Tensor Property Analysis ===\")\n",
    "\n",
    "# Create tensors of different dimensionalities\n",
    "scalar = torch.tensor(5.0)\n",
    "vector = torch.randn(10)\n",
    "matrix = torch.randn(3, 4)\n",
    "tensor_3d = torch.randn(2, 3, 4)\n",
    "tensor_4d = torch.randn(2, 3, 4, 5)\n",
    "\n",
    "tensors = [scalar, vector, matrix, tensor_3d, tensor_4d]\n",
    "names = ['Scalar', 'Vector', 'Matrix', '3D Tensor', '4D Tensor']\n",
    "\n",
    "for name, tensor in zip(names, tensors):\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Shape: {tensor.shape}\")\n",
    "    print(f\"  Dimensions: {tensor.ndim}\")\n",
    "    print(f\"  Elements: {tensor.numel()}\")\n",
    "    print(f\"  Data type: {tensor.dtype}\")\n",
    "    print(f\"  Device: {tensor.device}\")\n",
    "    print(f\"  Memory (bytes): {tensor.element_size() * tensor.numel()}\")\n",
    "    print(f\"  Requires grad: {tensor.requires_grad}\")\n",
    "    print(f\"  Is leaf: {tensor.is_leaf}\")\n",
    "\n",
    "# Accessing specific dimensions\n",
    "print(f\"\\n=== Dimension Access for 4D tensor ===\")\n",
    "for i in range(tensor_4d.ndim):\n",
    "    print(f\"Dimension {i}: {tensor_4d.size(i)}\")\n",
    "\n",
    "print(f\"Last dimension: {tensor_4d.size(-1)}\")\n",
    "print(f\"Second to last: {tensor_4d.size(-2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting compatibility analysis\n",
    "print(\"=== Broadcasting Compatibility Analysis ===\")\n",
    "\n",
    "def analyze_shape_compatibility(tensor1: torch.Tensor, tensor2: torch.Tensor) -> bool:\n",
    "    \"\"\"Analyze if two tensors can be broadcast together\"\"\"\n",
    "    shape1, shape2 = tensor1.shape, tensor2.shape\n",
    "    \n",
    "    print(f\"Tensor 1 shape: {shape1}\")\n",
    "    print(f\"Tensor 2 shape: {shape2}\")\n",
    "    \n",
    "    try:\n",
    "        result_shape = torch.broadcast_shapes(shape1, shape2)\n",
    "        print(f\"Broadcasting result shape: {result_shape}\")\n",
    "        print(\"✓ Shapes are compatible\")\n",
    "        return True\n",
    "    except RuntimeError as e:\n",
    "        print(f\"✗ Shapes incompatible: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test various shape combinations\n",
    "test_pairs = [\n",
    "    (torch.randn(3, 1), torch.randn(1, 4)),      # (3,1) + (1,4) -> (3,4)\n",
    "    (torch.randn(3, 4), torch.randn(4)),         # (3,4) + (4,) -> (3,4)\n",
    "    (torch.randn(2, 1, 4), torch.randn(3, 4)),   # (2,1,4) + (3,4) -> (2,3,4)\n",
    "    (torch.randn(2, 3), torch.randn(4, 5)),      # Incompatible\n",
    "]\n",
    "\n",
    "for i, (t1, t2) in enumerate(test_pairs):\n",
    "    print(f\"\\nTest {i+1}:\")\n",
    "    analyze_shape_compatibility(t1, t2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Memory Layout and Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Memory Layout and Strides Analysis ===\")\n",
    "\n",
    "def analyze_memory_layout(tensor: torch.Tensor, name: str) -> None:\n",
    "    \"\"\"Analyze tensor memory layout\"\"\"\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Shape: {tensor.shape}\")\n",
    "    print(f\"  Strides: {tensor.stride()}\")\n",
    "    print(f\"  Is contiguous: {tensor.is_contiguous()}\")\n",
    "    print(f\"  Storage size: {tensor.storage().size()}\")\n",
    "    print(f\"  Storage offset: {tensor.storage_offset()}\")\n",
    "    \n",
    "    # Memory efficiency calculation\n",
    "    theoretical_size = tensor.numel()\n",
    "    actual_storage = tensor.storage().size()\n",
    "    efficiency = theoretical_size / actual_storage if actual_storage > 0 else 0\n",
    "    print(f\"  Memory efficiency: {efficiency:.2%}\")\n",
    "    print()\n",
    "\n",
    "# Create different memory layouts\n",
    "original = torch.randn(4, 6)\n",
    "transposed = original.t()  # Transpose creates non-contiguous view\n",
    "contiguous = transposed.contiguous()  # Make contiguous copy\n",
    "sliced = original[:2, ::2]  # Slicing can create non-contiguous view\n",
    "\n",
    "analyze_memory_layout(original, \"Original (4,6)\")\n",
    "analyze_memory_layout(transposed, \"Transposed (6,4)\")\n",
    "analyze_memory_layout(contiguous, \"Contiguous copy\")\n",
    "analyze_memory_layout(sliced, \"Sliced view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance implications of memory layout\n",
    "print(\"=== Performance Impact of Memory Layout ===\")\n",
    "\n",
    "# Create large tensors for meaningful benchmarks\n",
    "large_tensor = torch.randn(1000, 1000)\n",
    "large_transposed = large_tensor.t()\n",
    "large_contiguous = large_transposed.contiguous()\n",
    "\n",
    "# Benchmark operations on different memory layouts\n",
    "operations = [\n",
    "    ('sum', lambda x: x.sum()),\n",
    "    ('mean', lambda x: x.mean()),\n",
    "    ('std', lambda x: x.std()),\n",
    "    ('matmul', lambda x: torch.mm(x[:100, :100], x[:100, :100]))\n",
    "]\n",
    "\n",
    "print(\"Performance comparison (100 iterations):\")\n",
    "for op_name, op_func in operations:\n",
    "    # Time contiguous tensor\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = op_func(large_contiguous)\n",
    "    contiguous_time = time.time() - start\n",
    "    \n",
    "    # Time non-contiguous tensor\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = op_func(large_transposed)\n",
    "    non_contiguous_time = time.time() - start\n",
    "    \n",
    "    print(f\"{op_name:8} - Contiguous: {contiguous_time:.4f}s, \"\n",
    "          f\"Non-contiguous: {non_contiguous_time:.4f}s, \"\n",
    "          f\"Ratio: {non_contiguous_time/contiguous_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Types and Device Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Comprehensive Data Type Analysis ===\")\n",
    "\n",
    "# Complete overview of PyTorch data types\n",
    "data_types = {\n",
    "    'int8': torch.int8,\n",
    "    'int16': torch.int16, \n",
    "    'int32': torch.int32,\n",
    "    'int64': torch.int64,\n",
    "    'uint8': torch.uint8,\n",
    "    'float16': torch.float16,\n",
    "    'bfloat16': torch.bfloat16,\n",
    "    'float32': torch.float32,\n",
    "    'float64': torch.float64,\n",
    "    'complex64': torch.complex64,\n",
    "    'complex128': torch.complex128,\n",
    "    'bool': torch.bool\n",
    "}\n",
    "\n",
    "print(\"PyTorch Data Types Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, dtype in data_types.items():\n",
    "    try:\n",
    "        if dtype == torch.bool:\n",
    "            tensor = torch.tensor([True, False, True], dtype=dtype)\n",
    "            print(f\"{name:10} - Size: {tensor.element_size():2d} bytes, Values: True/False\")\n",
    "        elif dtype in [torch.complex64, torch.complex128]:\n",
    "            tensor = torch.tensor([1+2j, 3+4j], dtype=dtype)\n",
    "            print(f\"{name:10} - Size: {tensor.element_size():2d} bytes, Complex type\")\n",
    "        else:\n",
    "            tensor = torch.tensor([1.0, 2.0, 3.0], dtype=dtype)\n",
    "            if dtype.is_floating_point:\n",
    "                info = torch.finfo(dtype)\n",
    "                print(f\"{name:10} - Size: {tensor.element_size():2d} bytes, \"\n",
    "                      f\"Range: {info.min:.2e} to {info.max:.2e}\")\n",
    "            else:\n",
    "                info = torch.iinfo(dtype)\n",
    "                print(f\"{name:10} - Size: {tensor.element_size():2d} bytes, \"\n",
    "                      f\"Range: {info.min:>12} to {info.max:>12}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name:10} - Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type promotion and precision analysis\n",
    "print(\"=== Type Promotion Analysis ===\")\n",
    "\n",
    "def demonstrate_type_promotion(tensor1: torch.Tensor, tensor2: torch.Tensor, operation_name: str) -> None:\n",
    "    \"\"\"Show how PyTorch promotes types during operations\"\"\"\n",
    "    print(f\"{operation_name}:\")\n",
    "    print(f\"  Input 1: {tensor1.dtype} = {tensor1}\")\n",
    "    print(f\"  Input 2: {tensor2.dtype} = {tensor2}\")\n",
    "    \n",
    "    result = tensor1 + tensor2\n",
    "    print(f\"  Result: {result.dtype} = {result}\")\n",
    "    print()\n",
    "\n",
    "# Test different type promotion scenarios\n",
    "test_cases = [\n",
    "    (torch.tensor([1], dtype=torch.int32), torch.tensor([2.0], dtype=torch.float32), \"int32 + float32\"),\n",
    "    (torch.tensor([1], dtype=torch.float16), torch.tensor([2.0], dtype=torch.float32), \"float16 + float32\"),\n",
    "    (torch.tensor([True], dtype=torch.bool), torch.tensor([5], dtype=torch.int32), \"bool + int32\"),\n",
    "]\n",
    "\n",
    "for t1, t2, desc in test_cases:\n",
    "    demonstrate_type_promotion(t1, t2, desc)\n",
    "\n",
    "print(\"Type Promotion Hierarchy (lower to higher):\")\n",
    "print(\"bool -> uint8 -> int8 -> int16 -> int32 -> int64\")\n",
    "print(\"         -> float16 -> float32 -> float64\")\n",
    "print(\"              -> complex64 -> complex128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced device management\n",
    "print(\"=== Advanced Device Management ===\")\n",
    "\n",
    "# Device information\n",
    "devices_to_test = ['cpu']\n",
    "if torch.cuda.is_available():\n",
    "    devices_to_test.append('cuda')\n",
    "if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    devices_to_test.append('mps')\n",
    "\n",
    "for device_name in devices_to_test:\n",
    "    print(f\"\\nTesting device: {device_name}\")\n",
    "    device = torch.device(device_name)\n",
    "    \n",
    "    # Create tensor on device\n",
    "    tensor_on_device = torch.randn(1000, 1000, device=device)\n",
    "    print(f\"  Tensor device: {tensor_on_device.device}\")\n",
    "    print(f\"  Tensor shape: {tensor_on_device.shape}\")\n",
    "    \n",
    "    # Memory usage (CUDA only)\n",
    "    if device_name == 'cuda':\n",
    "        allocated = torch.cuda.memory_allocated(device) / 1e6\n",
    "        cached = torch.cuda.memory_reserved(device) / 1e6\n",
    "        print(f\"  GPU memory allocated: {allocated:.1f} MB\")\n",
    "        print(f\"  GPU memory cached: {cached:.1f} MB\")\n",
    "\n",
    "# Device transfer performance benchmarking\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n=== Device Transfer Performance ===\")\n",
    "    \n",
    "    def benchmark_device_transfer(tensor_size: Tuple[int, ...], num_iterations: int = 10) -> None:\n",
    "        cpu_tensor = torch.randn(*tensor_size)\n",
    "        data_size_mb = cpu_tensor.element_size() * cpu_tensor.numel() / 1e6\n",
    "        \n",
    "        print(f\"Tensor size: {tensor_size}, Data: {data_size_mb:.1f} MB\")\n",
    "        \n",
    "        # CPU to GPU transfer\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        for _ in range(num_iterations):\n",
    "            gpu_tensor = cpu_tensor.to('cuda')\n",
    "            torch.cuda.synchronize()\n",
    "        cpu_to_gpu_time = time.time() - start\n",
    "        \n",
    "        # GPU to CPU transfer\n",
    "        gpu_tensor = cpu_tensor.to('cuda')\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        for _ in range(num_iterations):\n",
    "            cpu_copy = gpu_tensor.to('cpu')\n",
    "        gpu_to_cpu_time = time.time() - start\n",
    "        \n",
    "        bandwidth_to_gpu = (data_size_mb * num_iterations) / cpu_to_gpu_time\n",
    "        bandwidth_to_cpu = (data_size_mb * num_iterations) / gpu_to_cpu_time\n",
    "        \n",
    "        print(f\"  CPU -> GPU: {cpu_to_gpu_time:.3f}s ({bandwidth_to_gpu:.0f} MB/s)\")\n",
    "        print(f\"  GPU -> CPU: {gpu_to_cpu_time:.3f}s ({bandwidth_to_cpu:.0f} MB/s)\")\n",
    "        print()\n",
    "    \n",
    "    # Test different tensor sizes\n",
    "    test_sizes = [(100, 100), (1000, 1000), (100, 100, 100)]\n",
    "    for size in test_sizes:\n",
    "        benchmark_device_transfer(size)\n",
    "else:\n",
    "    print(\"\\nCUDA not available for device transfer benchmarking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Tensor Manipulation Techniques\n",
    "\n",
    "### 3.1 Advanced Reshaping with Memory Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Advanced Tensor Reshaping ===\")\n",
    "\n",
    "# view() vs reshape() comprehensive comparison\n",
    "original = torch.arange(24)\n",
    "print(f\"Original tensor: {original}\")\n",
    "print(f\"Original shape: {original.shape}\")\n",
    "print(f\"Is contiguous: {original.is_contiguous()}\")\n",
    "\n",
    "# Both view() and reshape() can change shape\n",
    "view_result = original.view(4, 6)\n",
    "reshape_result = original.reshape(4, 6)\n",
    "\n",
    "print(f\"\\nview(4, 6): {view_result.shape}\")\n",
    "print(f\"reshape(4, 6): {reshape_result.shape}\")\n",
    "print(f\"Results identical: {torch.equal(view_result, reshape_result)}\")\n",
    "print(f\"Share memory: {view_result.data_ptr() == original.data_ptr()}\")\n",
    "\n",
    "# Key difference: view() requires contiguous memory\n",
    "print(\"\\n=== Contiguity Requirements ===\")\n",
    "transposed = original.view(4, 6).t()  # Create non-contiguous tensor\n",
    "print(f\"Transposed is contiguous: {transposed.is_contiguous()}\")\n",
    "\n",
    "try:\n",
    "    view_transposed = transposed.view(24)  # This will fail\n",
    "except RuntimeError as e:\n",
    "    print(f\"view() error: {e}\")\n",
    "\n",
    "# reshape() works even with non-contiguous tensors\n",
    "reshape_transposed = transposed.reshape(24)\n",
    "print(f\"reshape() succeeded: shape {reshape_transposed.shape}\")\n",
    "print(f\"Is result contiguous: {reshape_transposed.is_contiguous()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced reshaping patterns for deep learning\n",
    "print(\"=== Deep Learning Reshaping Patterns ===\")\n",
    "\n",
    "# Image batch processing\n",
    "batch_data = torch.randn(32, 3, 224, 224)  # Typical image batch\n",
    "print(f\"Image batch: {batch_data.shape}\")\n",
    "\n",
    "# Flatten for fully connected layer\n",
    "flattened = batch_data.view(32, -1)\n",
    "print(f\"Flattened: {flattened.shape}\")\n",
    "print(f\"Calculated size: 3 * 224 * 224 = {3 * 224 * 224}\")\n",
    "\n",
    "# NCHW to NHWC conversion (PyTorch to TensorFlow format)\n",
    "nchw = torch.randn(8, 64, 32, 32)\n",
    "nhwc = nchw.permute(0, 2, 3, 1)\n",
    "print(f\"\\nNCHW format: {nchw.shape}\")\n",
    "print(f\"NHWC format: {nhwc.shape}\")\n",
    "\n",
    "# Sequence processing\n",
    "sequence_data = torch.randn(16, 100, 512)  # (batch, seq_len, features)\n",
    "print(f\"\\nSequence data: {sequence_data.shape}\")\n",
    "\n",
    "# Reshape for parallel processing of all sequences\n",
    "all_tokens = sequence_data.view(-1, 512)  # (batch*seq_len, features)\n",
    "print(f\"All tokens: {all_tokens.shape}\")\n",
    "\n",
    "# Reshape back\n",
    "reshaped_back = all_tokens.view(16, 100, 512)\n",
    "print(f\"Reshaped back: {reshaped_back.shape}\")\n",
    "print(f\"Data preserved: {torch.equal(sequence_data, reshaped_back)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient reshaping performance analysis\n",
    "print(\"=== Memory-Efficient Reshaping Analysis ===\")\n",
    "\n",
    "def analyze_reshape_performance(tensor: torch.Tensor, target_shape: Tuple[int, ...], method_name: str) -> None:\n",
    "    \"\"\"Analyze performance and memory efficiency of reshape operations\"\"\"\n",
    "    print(f\"{method_name}:\")\n",
    "    \n",
    "    start_mem = tensor.storage().size() * tensor.element_size()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    if method_name == 'view':\n",
    "        result = tensor.view(target_shape)\n",
    "    elif method_name == 'reshape':\n",
    "        result = tensor.reshape(target_shape)\n",
    "    elif method_name == 'contiguous+view':\n",
    "        result = tensor.contiguous().view(target_shape)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    end_mem = result.storage().size() * result.element_size()\n",
    "    \n",
    "    print(f\"  Time: {(end_time - start_time) * 1000:.4f} ms\")\n",
    "    print(f\"  Memory before: {start_mem:,} bytes\")\n",
    "    print(f\"  Memory after: {end_mem:,} bytes\")\n",
    "    print(f\"  Memory change: {end_mem - start_mem:,} bytes\")\n",
    "    print(f\"  Shares storage: {tensor.data_ptr() == result.data_ptr()}\")\n",
    "    print(f\"  Is contiguous: {result.is_contiguous()}\")\n",
    "    print()\n",
    "\n",
    "# Test with large tensor\n",
    "large_tensor = torch.randn(1000, 1000)\n",
    "target = (1000000,)  # Flatten\n",
    "\n",
    "analyze_reshape_performance(large_tensor, target, 'view')\n",
    "analyze_reshape_performance(large_tensor, target, 'reshape')\n",
    "\n",
    "# Test with non-contiguous tensor\n",
    "non_contiguous = large_tensor.t()\n",
    "print(\"Non-contiguous tensor operations:\")\n",
    "try:\n",
    "    analyze_reshape_performance(non_contiguous, target, 'view')\n",
    "except RuntimeError:\n",
    "    print(\"view: Failed due to non-contiguity\\n\")\n",
    "    \n",
    "analyze_reshape_performance(non_contiguous, target, 'reshape')\n",
    "analyze_reshape_performance(non_contiguous, target, 'contiguous+view')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Advanced Dimension Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Advanced Dimension Manipulation ===\")\n",
    "\n",
    "# squeeze() and unsqueeze() for broadcasting\n",
    "original = torch.tensor([1, 2, 3, 4])\n",
    "print(f\"Original: {original.shape} = {original}\")\n",
    "\n",
    "# Adding dimensions with unsqueeze()\n",
    "dimensions_added = {\n",
    "    'unsqueeze(0)': original.unsqueeze(0),    # Add batch dimension\n",
    "    'unsqueeze(1)': original.unsqueeze(1),    # Add feature dimension\n",
    "    'unsqueeze(-1)': original.unsqueeze(-1),  # Add trailing dimension\n",
    "    'multiple': original.unsqueeze(0).unsqueeze(2),  # Multiple dimensions\n",
    "}\n",
    "\n",
    "for desc, tensor in dimensions_added.items():\n",
    "    print(f\"{desc:20}: {tensor.shape}\")\n",
    "\n",
    "# Removing dimensions with squeeze()\n",
    "squeeze_examples = torch.randn(1, 4, 1, 3, 1)\n",
    "print(f\"\\nOriginal with size-1 dims: {squeeze_examples.shape}\")\n",
    "\n",
    "squeeze_operations = {\n",
    "    'squeeze()': squeeze_examples.squeeze(),           # Remove all size-1 dims\n",
    "    'squeeze(0)': squeeze_examples.squeeze(0),         # Remove specific dim\n",
    "    'squeeze(2)': squeeze_examples.squeeze(2),         # Remove another specific dim\n",
    "}\n",
    "\n",
    "for desc, result in squeeze_operations.items():\n",
    "    print(f\"{desc:12}: {result.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting-enabled operations for neural networks\n",
    "print(\"=== Broadcasting for Neural Networks ===\")\n",
    "\n",
    "# Batch normalization-style operation\n",
    "batch_features = torch.randn(32, 64, 28, 28)  # (batch, channels, height, width)\n",
    "print(f\"Batch features: {batch_features.shape}\")\n",
    "\n",
    "# Channel-wise statistics\n",
    "channel_mean = batch_features.mean(dim=[0, 2, 3])  # Mean over batch and spatial dims\n",
    "channel_std = batch_features.std(dim=[0, 2, 3])\n",
    "print(f\"Channel mean shape: {channel_mean.shape}\")\n",
    "print(f\"Channel std shape: {channel_std.shape}\")\n",
    "\n",
    "# Reshape for broadcasting\n",
    "mean_broadcast = channel_mean.unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "std_broadcast = channel_std.unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
    "print(f\"Mean for broadcasting: {mean_broadcast.shape}\")\n",
    "print(f\"Std for broadcasting: {std_broadcast.shape}\")\n",
    "\n",
    "# Apply normalization via broadcasting\n",
    "normalized = (batch_features - mean_broadcast) / (std_broadcast + 1e-8)\n",
    "print(f\"Normalized features: {normalized.shape}\")\n",
    "\n",
    "# Alternative using view for same effect\n",
    "mean_view = channel_mean.view(1, -1, 1, 1)\n",
    "std_view = channel_std.view(1, -1, 1, 1)\n",
    "normalized_alt = (batch_features - mean_view) / (std_view + 1e-8)\n",
    "print(f\"Results identical: {torch.allclose(normalized, normalized_alt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention reshaping (Transformer architecture)\n",
    "print(\"=== Multi-Head Attention Reshaping ===\")\n",
    "\n",
    "class MultiHeadAttentionReshaping:\n",
    "    def __init__(self, d_model: int = 512, n_heads: int = 8):\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Simulated weight matrices\n",
    "        self.W_q = torch.randn(d_model, d_model)\n",
    "        self.W_k = torch.randn(d_model, d_model)\n",
    "        self.W_v = torch.randn(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = torch.matmul(x, self.W_q)\n",
    "        K = torch.matmul(x, self.W_k)\n",
    "        V = torch.matmul(x, self.W_v)\n",
    "        print(f\"After linear projection: Q={Q.shape}, K={K.shape}, V={V.shape}\")\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        print(f\"Multi-head reshape: Q={Q.shape}, K={K.shape}, V={V.shape}\")\n",
    "        \n",
    "        # Attention computation (simplified)\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        attention_scores /= (self.d_k ** 0.5)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        print(f\"Attention weights: {attention_weights.shape}\")\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        print(f\"Attention output: {attention_output.shape}\")\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "        print(f\"Final output: {attention_output.shape}\")\n",
    "        \n",
    "        return attention_output\n",
    "\n",
    "# Example usage\n",
    "batch_size, seq_len, d_model = 4, 16, 512\n",
    "input_tensor = torch.randn(batch_size, seq_len, d_model)\n",
    "attention = MultiHeadAttentionReshaping(d_model, n_heads=8)\n",
    "\n",
    "output = attention.forward(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Indexing and Slicing Techniques\n",
    "\n",
    "### 4.1 Comprehensive Indexing Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Advanced Indexing Techniques ===\")\n\n# Create comprehensive test tensor\ntensor_3d = torch.arange(120).view(4, 5, 6)\nprint(f\"Original tensor shape: {tensor_3d.shape}\")\nprint(f\"First matrix:\\n{tensor_3d[0]}\")\n\n# Basic indexing patterns\nprint(f\"\\n=== Basic Indexing ===\")\nprint(f\"Single element [1,2,3]: {tensor_3d[1, 2, 3]}\")\nprint(f\"Row [0,1]: {tensor_3d[0, 1]}\")\nprint(f\"Column [:,:,0] shape: {tensor_3d[:, :, 0].shape}\")\n\n# Advanced slicing\nprint(f\"\\n=== Advanced Slicing ===\")\nprint(f\"First two batches: {tensor_3d[:2].shape}\")\nprint(f\"Every other element in last dim: {tensor_3d[0, 0, ::2]}\")\n\n# PyTorch 2.x supports negative step, but let's use a more explicit approach\nlast_dim_reversed = tensor_3d[0, 0].flip(0)\nprint(f\"Reverse order: {last_dim_reversed}\")\n\nprint(f\"Complex slice [:, 1:4, 2:5]: {tensor_3d[:, 1:4, 2:5].shape}\")\n\n# Negative indexing\nprint(f\"\\n=== Negative Indexing ===\")\nprint(f\"Last element: {tensor_3d[-1, -1, -1]}\")\nprint(f\"Last row of first batch: {tensor_3d[0, -1]}\")\nprint(f\"Last two in each dim: {tensor_3d[-2:, -2:, -2:].shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean indexing and masking\n",
    "print(\"=== Boolean Indexing and Masking ===\")\n",
    "\n",
    "# Create test data\n",
    "data = torch.randn(100)\n",
    "positive_mask = data > 0\n",
    "positive_values = data[positive_mask]\n",
    "\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "print(f\"Positive mask sum: {positive_mask.sum().item()} positive values\")\n",
    "print(f\"Positive values shape: {positive_values.shape}\")\n",
    "print(f\"First 5 positive values: {positive_values[:5]}\")\n",
    "\n",
    "# Complex boolean operations\n",
    "large_positive = (data > 0.5) & (data < 2.0)\n",
    "extreme_values = (data < -1.0) | (data > 1.5)\n",
    "print(f\"\\nLarge positive count: {large_positive.sum().item()}\")\n",
    "print(f\"Extreme values count: {extreme_values.sum().item()}\")\n",
    "\n",
    "# Advanced masking for attention\n",
    "seq_len = 8\n",
    "attention_scores = torch.randn(seq_len, seq_len)\n",
    "\n",
    "# Create causal mask (lower triangular)\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len), diagonal=0).bool()\n",
    "print(f\"\\nCausal mask shape: {causal_mask.shape}\")\n",
    "print(f\"Causal mask:\\n{causal_mask.int()}\")\n",
    "\n",
    "# Apply mask\n",
    "masked_scores = attention_scores.clone()\n",
    "masked_scores[~causal_mask] = float('-inf')\n",
    "print(f\"\\nMasked attention scores (first 3x3):\\n{masked_scores[:3, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced tensor indexing with integer arrays\n",
    "print(\"=== Advanced Tensor Indexing ===\")\n",
    "\n",
    "# Fancy indexing with tensors\n",
    "matrix = torch.arange(20).view(4, 5)\n",
    "row_indices = torch.tensor([0, 2, 1, 3])\n",
    "col_indices = torch.tensor([1, 3, 0, 4])\n",
    "\n",
    "print(f\"Matrix shape: {matrix.shape}\")\n",
    "print(f\"Matrix:\\n{matrix}\")\n",
    "print(f\"Selected elements: {matrix[row_indices, col_indices]}\")\n",
    "\n",
    "# Broadcasting indexing\n",
    "broadcast_result = matrix[row_indices[:, None], col_indices]\n",
    "print(f\"Broadcasting indexing shape: {broadcast_result.shape}\")\n",
    "print(f\"Broadcasting result:\\n{broadcast_result}\")\n",
    "\n",
    "# Variable-length sequence indexing\n",
    "batch_tensor = torch.randn(8, 10, 64)  # (batch, seq_len, features)\n",
    "sequence_lengths = torch.tensor([7, 9, 5, 8, 6, 10, 4, 8])\n",
    "\n",
    "# Extract last valid element for each sequence\n",
    "batch_indices = torch.arange(8)\n",
    "last_indices = sequence_lengths - 1\n",
    "last_elements = batch_tensor[batch_indices, last_indices]  # (8, 64)\n",
    "print(f\"\\nLast elements shape: {last_elements.shape}\")\n",
    "\n",
    "# Gather and scatter operations\n",
    "source_tensor = torch.randn(5, 8)\n",
    "indices = torch.tensor([[0, 2, 4], [1, 3, 7], [0, 1, 2]])\n",
    "gathered = torch.gather(source_tensor, 1, indices)\n",
    "\n",
    "print(f\"\\nGather operation:\")\n",
    "print(f\"Source shape: {source_tensor.shape}\")\n",
    "print(f\"Indices shape: {indices.shape}\")\n",
    "print(f\"Gathered shape: {gathered.shape}\")\n",
    "\n",
    "# Scatter operation (reverse of gather)\n",
    "target = torch.zeros(5, 8)\n",
    "target.scatter_(1, indices, gathered)\n",
    "print(f\"Scattered target shape: {target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Optimization and Best Practices\n",
    "\n",
    "### 5.1 Memory Access Patterns and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Performance Optimization Techniques ===\")\n",
    "\n",
    "# Memory access pattern analysis\n",
    "def benchmark_access_patterns() -> None:\n",
    "    \"\"\"Benchmark different memory access patterns\"\"\"\n",
    "    large_tensor = torch.randn(1000, 1000, device='cpu')\n",
    "    \n",
    "    # Row-wise access (cache-friendly)\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        result = large_tensor.sum(dim=1)\n",
    "    row_wise_time = time.time() - start\n",
    "    \n",
    "    # Column-wise access (cache-unfriendly)\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        result = large_tensor.sum(dim=0)\n",
    "    col_wise_time = time.time() - start\n",
    "    \n",
    "    print(f\"Row-wise access time: {row_wise_time:.4f}s\")\n",
    "    print(f\"Column-wise access time: {col_wise_time:.4f}s\")\n",
    "    print(f\"Column/Row ratio: {col_wise_time/row_wise_time:.2f}x slower\")\n",
    "    \n",
    "benchmark_access_patterns()\n",
    "\n",
    "# Vectorization vs loops comparison\n",
    "def compare_vectorization() -> None:\n",
    "    \"\"\"Compare vectorized vs loop operations\"\"\"\n",
    "    data = torch.randn(10000)\n",
    "    \n",
    "    # Vectorized operation\n",
    "    start = time.time()\n",
    "    for _ in range(1000):\n",
    "        result_vec = torch.where(data > 0, data * 2, data * 0.5)\n",
    "    vec_time = time.time() - start\n",
    "    \n",
    "    # Manual loop (much slower - for demonstration only)\n",
    "    start = time.time()\n",
    "    result_loop = torch.zeros_like(data)\n",
    "    for i in range(min(1000, data.size(0))):\n",
    "        if data[i] > 0:\n",
    "            result_loop[i] = data[i] * 2\n",
    "        else:\n",
    "            result_loop[i] = data[i] * 0.5\n",
    "    loop_time = time.time() - start\n",
    "    \n",
    "    print(f\"\\nVectorization vs Loops:\")\n",
    "    print(f\"Vectorized time: {vec_time:.4f}s\")\n",
    "    print(f\"Loop time (1000 elements): {loop_time:.4f}s\")\n",
    "    print(f\"Estimated speedup for full array: {loop_time/vec_time*10:.0f}x\")\n",
    "    \n",
    "compare_vectorization()\n",
    "\n",
    "# In-place vs out-of-place operations\n",
    "def compare_inplace_operations() -> None:\n",
    "    \"\"\"Compare in-place vs out-of-place operations\"\"\"\n",
    "    # Out-of-place operations\n",
    "    tensor1 = torch.randn(1000, 1000)\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        tensor1 = tensor1 + 1  # Creates new tensor each time\n",
    "    out_of_place_time = time.time() - start\n",
    "    \n",
    "    # In-place operations\n",
    "    tensor2 = torch.randn(1000, 1000)\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        tensor2 += 1  # Modifies existing tensor\n",
    "    in_place_time = time.time() - start\n",
    "    \n",
    "    print(f\"\\nIn-place vs Out-of-place:\")\n",
    "    print(f\"Out-of-place time: {out_of_place_time:.4f}s\")\n",
    "    print(f\"In-place time: {in_place_time:.4f}s\")\n",
    "    print(f\"In-place speedup: {out_of_place_time/in_place_time:.2f}x\")\n",
    "    \n",
    "compare_inplace_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting efficiency analysis\n",
    "print(\"=== Broadcasting Efficiency Analysis ===\")\n",
    "\n",
    "def analyze_broadcasting_efficiency() -> None:\n",
    "    \"\"\"Analyze efficiency of different broadcasting patterns\"\"\"\n",
    "    batch_size, seq_len, d_model = 32, 128, 512\n",
    "    \n",
    "    query = torch.randn(batch_size, seq_len, d_model)\n",
    "    key = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Method 1: Batch matrix multiplication\n",
    "    start = time.time()\n",
    "    scores_bmm = torch.bmm(query, key.transpose(1, 2))\n",
    "    bmm_time = time.time() - start\n",
    "    \n",
    "    # Method 2: Efficient einsum\n",
    "    start = time.time()\n",
    "    scores_einsum = torch.einsum('bqd,bkd->bqk', query, key)\n",
    "    einsum_time = time.time() - start\n",
    "    \n",
    "    # Method 3: Manual broadcasting (less efficient)\n",
    "    start = time.time()\n",
    "    q_expanded = query.unsqueeze(3)  # (batch, seq, d_model, 1)\n",
    "    k_expanded = key.unsqueeze(2)    # (batch, 1, seq, d_model)\n",
    "    scores_manual = (q_expanded * k_expanded).sum(dim=1)  # Less efficient\n",
    "    manual_time = time.time() - start\n",
    "    \n",
    "    print(f\"BMM method: {bmm_time:.4f}s\")\n",
    "    print(f\"Einsum method: {einsum_time:.4f}s\")\n",
    "    print(f\"Manual broadcasting: {manual_time:.4f}s\")\n",
    "    \n",
    "    # Verify results are equivalent\n",
    "    print(f\"\\nResults verification:\")\n",
    "    print(f\"BMM vs Einsum: {torch.allclose(scores_bmm, scores_einsum, atol=1e-5)}\")\n",
    "    \n",
    "    # Memory usage comparison\n",
    "    bmm_mem = scores_bmm.element_size() * scores_bmm.nelement()\n",
    "    manual_mem = (q_expanded.element_size() * q_expanded.nelement() + \n",
    "                  k_expanded.element_size() * k_expanded.nelement())\n",
    "    \n",
    "    print(f\"\\nMemory usage:\")\n",
    "    print(f\"BMM result: {bmm_mem / 1e6:.1f} MB\")\n",
    "    print(f\"Manual intermediate: {manual_mem / 1e6:.1f} MB\")\n",
    "    print(f\"Memory efficiency: {manual_mem / bmm_mem:.1f}x more memory for manual\")\n",
    "\n",
    "analyze_broadcasting_efficiency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Advanced Performance Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch compilation for performance (if available)\n",
    "print(\"=== Advanced Performance Techniques ===\")\n",
    "\n",
    "def demonstrate_torch_compile() -> None:\n",
    "    \"\"\"Demonstrate torch.compile for performance optimization\"\"\"\n",
    "    try:\n",
    "        # Simple function to compile\n",
    "        def matrix_operations(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "            z = torch.matmul(x, y)\n",
    "            z = torch.relu(z)\n",
    "            z = torch.softmax(z, dim=-1)\n",
    "            return z\n",
    "        \n",
    "        # Try to compile the function (PyTorch 2.0+)\n",
    "        if hasattr(torch, 'compile'):\n",
    "            compiled_fn = torch.compile(matrix_operations)\n",
    "            \n",
    "            # Test data\n",
    "            x = torch.randn(500, 512)\n",
    "            y = torch.randn(512, 500)\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(5):\n",
    "                _ = matrix_operations(x, y)\n",
    "                _ = compiled_fn(x, y)\n",
    "            \n",
    "            # Benchmark\n",
    "            start = time.time()\n",
    "            for _ in range(50):\n",
    "                result_regular = matrix_operations(x, y)\n",
    "            regular_time = time.time() - start\n",
    "            \n",
    "            start = time.time()\n",
    "            for _ in range(50):\n",
    "                result_compiled = compiled_fn(x, y)\n",
    "            compiled_time = time.time() - start\n",
    "            \n",
    "            print(f\"Torch Compile Performance:\")\n",
    "            print(f\"Regular execution: {regular_time:.4f}s\")\n",
    "            print(f\"Compiled execution: {compiled_time:.4f}s\")\n",
    "            print(f\"Compile speedup: {regular_time/compiled_time:.2f}x\")\n",
    "            print(f\"Results identical: {torch.allclose(result_regular, result_compiled)}\")\n",
    "        else:\n",
    "            print(\"torch.compile not available (requires PyTorch 2.0+)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Torch compile error: {e}\")\n",
    "        print(\"This requires PyTorch 2.0+ and appropriate backend support\")\n",
    "\n",
    "demonstrate_torch_compile()\n",
    "\n",
    "# Memory-efficient batch processing\n",
    "def demonstrate_efficient_batch_processing() -> None:\n",
    "    \"\"\"Demonstrate memory-efficient batch processing techniques\"\"\"\n",
    "    print(f\"\\n=== Memory-Efficient Batch Processing ===\")\n",
    "    \n",
    "    # Simulate large dataset processing\n",
    "    total_samples = 10000\n",
    "    feature_dim = 512\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Generate fake data\n",
    "    data = torch.randn(total_samples, feature_dim)\n",
    "    \n",
    "    print(f\"Total data: {data.shape}\")\n",
    "    print(f\"Memory usage: {data.element_size() * data.nelement() / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Efficient batch processing with minimal memory overhead\n",
    "    def process_in_batches(data: torch.Tensor, batch_size: int) -> torch.Tensor:\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, data.size(0), batch_size):\n",
    "            batch = data[i:i+batch_size]\n",
    "            \n",
    "            # Simulate processing (e.g., neural network forward pass)\n",
    "            processed = torch.softmax(batch @ batch.t(), dim=-1)\n",
    "            \n",
    "            # Store only what we need\n",
    "            results.append(processed.mean(dim=1))\n",
    "        \n",
    "        return torch.cat(results, dim=0)\n",
    "    \n",
    "    start = time.time()\n",
    "    batch_results = process_in_batches(data, batch_size)\n",
    "    batch_time = time.time() - start\n",
    "    \n",
    "    print(f\"Batch processing time: {batch_time:.4f}s\")\n",
    "    print(f\"Result shape: {batch_results.shape}\")\n",
    "    print(f\"Peak memory efficient: Processing {batch_size} samples at a time\")\n",
    "\n",
    "demonstrate_efficient_batch_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-World Applications and Examples\n",
    "\n",
    "### 6.1 Complete Character-Level Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Character-Level Language Model Implementation ===\")\n\nclass CharacterLevelMLP:\n    \"\"\"Simple character-level language model using tensor operations\"\"\"\n    \n    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, generator=None):\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.generator = generator or torch.Generator().manual_seed(42)\n        \n        # Initialize parameters using advanced techniques\n        self.embedding = torch.randn(vocab_size, embedding_dim, generator=self.generator) * 0.1\n        \n        # Kaiming initialization for ReLU networks\n        self.W1 = self._kaiming_init(embedding_dim, hidden_dim)\n        self.b1 = torch.zeros(hidden_dim)\n        \n        self.W2 = self._kaiming_init(hidden_dim, hidden_dim)\n        self.b2 = torch.zeros(hidden_dim)\n        \n        # Output layer with smaller initialization\n        self.W_out = torch.randn(hidden_dim, vocab_size, generator=self.generator) * 0.01\n        self.b_out = torch.zeros(vocab_size)\n        \n        # Set requires_grad for all parameters\n        self.parameters = [self.embedding, self.W1, self.b1, self.W2, self.b2, self.W_out, self.b_out]\n        for p in self.parameters:\n            p.requires_grad_(True)\n        \n        self._print_initialization_stats()\n    \n    def _kaiming_init(self, in_features: int, out_features: int) -> torch.Tensor:\n        \"\"\"Kaiming initialization for ReLU networks\"\"\"\n        W = torch.randn(out_features, in_features, generator=self.generator)\n        fan_in = in_features\n        std = (2.0 / fan_in) ** 0.5\n        W *= std\n        return W\n    \n    def _print_initialization_stats(self) -> None:\n        \"\"\"Print parameter initialization statistics\"\"\"\n        print(f\"Character-Level MLP Initialization:\")\n        print(f\"  Vocab size: {self.vocab_size}\")\n        print(f\"  Embedding dim: {self.embedding_dim}\")\n        print(f\"  Hidden dim: {self.hidden_dim}\")\n        \n        param_names = ['Embedding', 'W1', 'b1', 'W2', 'b2', 'W_out', 'b_out']\n        print(f\"\\nParameter Statistics:\")\n        for name, param in zip(param_names, self.parameters):\n            print(f\"  {name:10} - Shape: {str(list(param.shape)):15} \"\n                  f\"Mean: {param.mean().item():6.3f} Std: {param.std().item():6.3f}\")\n        \n        total_params = sum(p.numel() for p in self.parameters)\n        print(f\"\\nTotal parameters: {total_params:,}\")\n    \n    def forward(self, indices: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward pass through the network\"\"\"\n        # Embedding lookup\n        x = self.embedding[indices]  # (batch_size, embedding_dim)\n        \n        # First hidden layer\n        h1 = torch.relu(x @ self.W1.t() + self.b1)\n        \n        # Second hidden layer\n        h2 = torch.relu(h1 @ self.W2.t() + self.b2)\n        \n        # Output layer - Fixed dimension issue\n        logits = h2 @ self.W_out + self.b_out\n        \n        return logits\n\n# Create and test the model\nvocab_size, embedding_dim, hidden_dim = 27, 32, 64\nmodel = CharacterLevelMLP(vocab_size, embedding_dim, hidden_dim)\n\n# Test forward pass\nbatch_size = 8\ntest_indices = torch.randint(0, vocab_size, (batch_size,))\nlogits = model.forward(test_indices)\n\nprint(f\"\\nForward pass test:\")\nprint(f\"Input indices: {test_indices}\")\nprint(f\"Output logits shape: {logits.shape}\")\nprint(f\"Output probabilities shape: {torch.softmax(logits, dim=-1).shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram dataset creation using advanced tensor operations\n",
    "print(\"=== Bigram Dataset Creation ===\")\n",
    "\n",
    "def create_bigram_dataset(words: List[str]) -> Tuple[torch.Tensor, torch.Tensor, dict, dict]:\n",
    "    \"\"\"Create bigram training dataset from words\"\"\"\n",
    "    # Create vocabulary\n",
    "    chars = sorted(list(set(''.join(words))))\n",
    "    stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "    stoi['.'] = 0  # Special start/end token\n",
    "    itos = {i: s for s, i in stoi.items()}\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(stoi)}\")\n",
    "    print(f\"Characters: {''.join(chars)}\")\n",
    "    \n",
    "    # Create bigram pairs\n",
    "    xs, ys = [], []\n",
    "    \n",
    "    for word in words:\n",
    "        chars_seq = ['.'] + list(word) + ['.']\n",
    "        for ch1, ch2 in zip(chars_seq, chars_seq[1:]):\n",
    "            ix1, ix2 = stoi[ch1], stoi[ch2]\n",
    "            xs.append(ix1)\n",
    "            ys.append(ix2)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X = torch.tensor(xs, dtype=torch.long)\n",
    "    Y = torch.tensor(ys, dtype=torch.long)\n",
    "    \n",
    "    return X, Y, stoi, itos\n",
    "\n",
    "def create_bigram_counts(X: torch.Tensor, Y: torch.Tensor, vocab_size: int) -> torch.Tensor:\n",
    "    \"\"\"Create bigram count matrix\"\"\"\n",
    "    N = torch.zeros((vocab_size, vocab_size), dtype=torch.int32)\n",
    "    \n",
    "    # Use advanced indexing to fill count matrix\n",
    "    for i in range(X.shape[0]):\n",
    "        N[X[i], Y[i]] += 1\n",
    "    \n",
    "    return N\n",
    "\n",
    "# Example with sample data\n",
    "sample_words = [\"emma\", \"olivia\", \"ava\", \"sophia\", \"isabella\", \"mia\", \"charlotte\"]\n",
    "X, Y, stoi, itos = create_bigram_dataset(sample_words)\n",
    "\n",
    "print(f\"\\nTraining data:\")\n",
    "print(f\"X (inputs) shape: {X.shape}\")\n",
    "print(f\"Y (targets) shape: {Y.shape}\")\n",
    "print(f\"First 10 bigrams: {[(itos[x.item()], itos[y.item()]) for x, y in zip(X[:10], Y[:10])]}\")\n",
    "\n",
    "# Create count matrix\n",
    "bigram_counts = create_bigram_counts(X, Y, len(stoi))\n",
    "print(f\"\\nBigram counts shape: {bigram_counts.shape}\")\n",
    "print(f\"Total bigrams: {bigram_counts.sum()}\")\n",
    "print(f\"Non-zero entries: {(bigram_counts > 0).sum()}\")\n",
    "print(f\"Sparsity: {1 - (bigram_counts > 0).float().mean():.3f}\")\n",
    "\n",
    "# Convert to probabilities using advanced tensor operations\n",
    "P = bigram_counts.float()\n",
    "P = P / P.sum(dim=1, keepdim=True)  # Normalize rows\n",
    "P = torch.nan_to_num(P)  # Handle division by zero\n",
    "\n",
    "print(f\"\\nProbability matrix (first 5x5):\\n{P[:5, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Attention Mechanism Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Attention Mechanism Implementation ===\")\n",
    "\n",
    "class ScaledDotProductAttention:\n",
    "    \"\"\"Scaled dot-product attention using advanced tensor operations\"\"\"\n",
    "    \n",
    "    def __init__(self, d_k: int, dropout_rate: float = 0.1):\n",
    "        self.d_k = d_k\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.scale = 1.0 / math.sqrt(d_k)\n",
    "    \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, \n",
    "                mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass of scaled dot-product attention\"\"\"\n",
    "        batch_size, seq_len, d_k = Q.shape\n",
    "        \n",
    "        print(f\"Input shapes: Q={Q.shape}, K={K.shape}, V={V.shape}\")\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.bmm(Q, K.transpose(1, 2)) * self.scale\n",
    "        print(f\"Attention scores shape: {scores.shape}\")\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            print(f\"Applying mask: {mask.shape}\")\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.bmm(attention_weights, V)\n",
    "        print(f\"Context shape: {context.shape}\")\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "# Create test data\n",
    "batch_size, seq_len, d_model = 4, 8, 64\n",
    "d_k = d_model\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "# Create causal mask\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "# Apply attention\n",
    "attention = ScaledDotProductAttention(d_k)\n",
    "context, weights = attention.forward(Q, K, V, causal_mask)\n",
    "\n",
    "print(f\"\\nAttention applied successfully!\")\n",
    "print(f\"Context statistics: mean={context.mean():.4f}, std={context.std():.4f}\")\n",
    "print(f\"Attention weights sum (should be 1.0): {weights.sum(dim=-1)[0, 0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization and Analysis\n",
    "\n",
    "### 7.1 Comprehensive Tensor Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Advanced Tensor Visualization ===\")\n",
    "\n",
    "# Create comprehensive visualization of tensor operations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# 1. Data type precision comparison\n",
    "x = torch.linspace(-5, 5, 1000)\n",
    "y_float64 = torch.sin(x).to(torch.float64)\n",
    "y_float32 = torch.sin(x).to(torch.float32)\n",
    "y_float16 = torch.sin(x).to(torch.float16)\n",
    "\n",
    "axes[0, 0].plot(x.numpy(), y_float64.numpy(), label='float64', linewidth=2)\n",
    "axes[0, 0].plot(x.numpy(), y_float32.numpy(), label='float32', linewidth=2, alpha=0.8)\n",
    "axes[0, 0].plot(x.numpy(), y_float16.numpy(), label='float16', linewidth=2, alpha=0.6)\n",
    "axes[0, 0].set_title('Data Type Precision Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Broadcasting visualization\n",
    "a = torch.arange(5).unsqueeze(1)  # (5, 1)\n",
    "b = torch.arange(4).unsqueeze(0)  # (1, 4)\n",
    "c = a + b  # Broadcasting result (5, 4)\n",
    "\n",
    "im = axes[0, 1].imshow(c.numpy(), cmap='viridis')\n",
    "axes[0, 1].set_title('Broadcasting Result (5,1) + (1,4)')\n",
    "plt.colorbar(im, ax=axes[0, 1])\n",
    "\n",
    "# 3. Attention weights visualization\n",
    "seq_len = 10\n",
    "attention_weights = torch.softmax(torch.randn(seq_len, seq_len), dim=-1)\n",
    "im = axes[0, 2].imshow(attention_weights.numpy(), cmap='Blues')\n",
    "axes[0, 2].set_title('Attention Weights Matrix')\n",
    "axes[0, 2].set_xlabel('Key Position')\n",
    "axes[0, 2].set_ylabel('Query Position')\n",
    "plt.colorbar(im, ax=axes[0, 2])\n",
    "\n",
    "# 4. Memory layout impact\n",
    "sizes = [100, 200, 500, 1000, 2000]\n",
    "contiguous_times = []\n",
    "non_contiguous_times = []\n",
    "\n",
    "for size in sizes:\n",
    "    tensor = torch.randn(size, size)\n",
    "    transposed = tensor.t()\n",
    "    \n",
    "    # Time contiguous\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = tensor.sum()\n",
    "    contiguous_times.append(time.time() - start)\n",
    "    \n",
    "    # Time non-contiguous\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = transposed.sum()\n",
    "    non_contiguous_times.append(time.time() - start)\n",
    "\n",
    "axes[1, 0].plot(sizes, contiguous_times, 'o-', label='Contiguous', linewidth=2)\n",
    "axes[1, 0].plot(sizes, non_contiguous_times, 's-', label='Non-contiguous', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Tensor Size')\n",
    "axes[1, 0].set_ylabel('Time (seconds)')\n",
    "axes[1, 0].set_title('Memory Layout Performance Impact')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Initialization method comparison\n",
    "methods = ['Standard', 'Xavier', 'Kaiming']\n",
    "layer_sizes = [(784, 256), (256, 128), (128, 64), (64, 10)]\n",
    "\n",
    "std_stds = []\n",
    "xavier_stds = []\n",
    "kaiming_stds = []\n",
    "\n",
    "for in_feat, out_feat in layer_sizes:\n",
    "    # Standard\n",
    "    std_W = torch.randn(out_feat, in_feat)\n",
    "    std_stds.append(std_W.std().item())\n",
    "    \n",
    "    # Xavier\n",
    "    xavier_W = torch.randn(out_feat, in_feat) * ((2.0 / (in_feat + out_feat)) ** 0.5)\n",
    "    xavier_stds.append(xavier_W.std().item())\n",
    "    \n",
    "    # Kaiming\n",
    "    kaiming_W = torch.randn(out_feat, in_feat) * ((2.0 / in_feat) ** 0.5)\n",
    "    kaiming_stds.append(kaiming_W.std().item())\n",
    "\n",
    "x_pos = range(len(layer_sizes))\n",
    "width = 0.25\n",
    "\n",
    "axes[1, 1].bar([x - width for x in x_pos], std_stds, width, label='Standard', alpha=0.8)\n",
    "axes[1, 1].bar(x_pos, xavier_stds, width, label='Xavier', alpha=0.8)\n",
    "axes[1, 1].bar([x + width for x in x_pos], kaiming_stds, width, label='Kaiming', alpha=0.8)\n",
    "\n",
    "axes[1, 1].set_xlabel('Layer')\n",
    "axes[1, 1].set_ylabel('Standard Deviation')\n",
    "axes[1, 1].set_title('Initialization Method Comparison')\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels([f'L{i+1}' for i in range(len(layer_sizes))])\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Tensor operation performance comparison\n",
    "operations = ['Sum', 'Mean', 'MatMul', 'Softmax']\n",
    "cpu_times = []\n",
    "gpu_times = []\n",
    "\n",
    "test_tensor = torch.randn(1000, 1000)\n",
    "\n",
    "# CPU times\n",
    "for op in operations:\n",
    "    start = time.time()\n",
    "    if op == 'Sum':\n",
    "        for _ in range(100): _ = test_tensor.sum()\n",
    "    elif op == 'Mean':\n",
    "        for _ in range(100): _ = test_tensor.mean()\n",
    "    elif op == 'MatMul':\n",
    "        for _ in range(100): _ = torch.mm(test_tensor[:100, :100], test_tensor[:100, :100])\n",
    "    elif op == 'Softmax':\n",
    "        for _ in range(100): _ = torch.softmax(test_tensor[:100, :100], dim=-1)\n",
    "    cpu_times.append(time.time() - start)\n",
    "\n",
    "# GPU times (if available)\n",
    "if torch.cuda.is_available():\n",
    "    test_tensor_gpu = test_tensor.to('cuda')\n",
    "    for op in operations:\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        if op == 'Sum':\n",
    "            for _ in range(100): _ = test_tensor_gpu.sum()\n",
    "        elif op == 'Mean':\n",
    "            for _ in range(100): _ = test_tensor_gpu.mean()\n",
    "        elif op == 'MatMul':\n",
    "            for _ in range(100): _ = torch.mm(test_tensor_gpu[:100, :100], test_tensor_gpu[:100, :100])\n",
    "        elif op == 'Softmax':\n",
    "            for _ in range(100): _ = torch.softmax(test_tensor_gpu[:100, :100], dim=-1)\n",
    "        torch.cuda.synchronize()\n",
    "        gpu_times.append(time.time() - start)\n",
    "else:\n",
    "    gpu_times = [0] * len(operations)\n",
    "\n",
    "x_pos = range(len(operations))\n",
    "axes[1, 2].bar([x - 0.2 for x in x_pos], cpu_times, 0.4, label='CPU', alpha=0.8)\n",
    "if torch.cuda.is_available():\n",
    "    axes[1, 2].bar([x + 0.2 for x in x_pos], gpu_times, 0.4, label='GPU', alpha=0.8)\n",
    "\n",
    "axes[1, 2].set_xlabel('Operation')\n",
    "axes[1, 2].set_ylabel('Time (seconds)')\n",
    "axes[1, 2].set_title('CPU vs GPU Performance')\n",
    "axes[1, 2].set_xticks(x_pos)\n",
    "axes[1, 2].set_xticklabels(operations)\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Comprehensive tensor visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Best Practices\n",
    "\n",
    "### Key Concepts Mastered\n",
    "\n",
    "In this comprehensive notebook, we have covered:\n",
    "\n",
    "1. **Advanced Tensor Creation**:\n",
    "   - `torch.tensor()` with memory sharing analysis\n",
    "   - `torch.zeros()` for neural network applications\n",
    "   - `torch.randn()` with advanced initialization techniques\n",
    "   - `torch.arange()` for position embeddings and coordinate grids\n",
    "\n",
    "2. **Tensor Properties and Memory Management**:\n",
    "   - Comprehensive shape and broadcasting analysis\n",
    "   - Memory layout, strides, and performance implications\n",
    "   - Data type precision and type promotion\n",
    "   - Advanced device management and transfer optimization\n",
    "\n",
    "3. **Advanced Tensor Manipulation**:\n",
    "   - Memory-efficient reshaping with `view()` vs `reshape()`\n",
    "   - Dimension manipulation for broadcasting\n",
    "   - Complex permutations for neural network architectures\n",
    "   - Real-world examples from Transformer models\n",
    "\n",
    "4. **Advanced Indexing and Slicing**:\n",
    "   - Boolean indexing and masking for attention mechanisms\n",
    "   - Tensor indexing with gather/scatter operations\n",
    "   - Variable-length sequence processing\n",
    "   - Conditional operations and advanced selection\n",
    "\n",
    "5. **Performance Optimization**:\n",
    "   - Memory access pattern analysis\n",
    "   - Vectorization vs loop performance\n",
    "   - Broadcasting efficiency optimization\n",
    "   - PyTorch compilation techniques\n",
    "\n",
    "6. **Real-World Applications**:\n",
    "   - Character-level language model implementation\n",
    "   - Attention mechanism with masking\n",
    "   - Multi-head attention reshaping\n",
    "   - Efficient batch processing techniques\n",
    "\n",
    "### Best Practices Learned\n",
    "\n",
    "1. **Memory Efficiency**:\n",
    "   - Use `torch.from_numpy()` when memory sharing is desired\n",
    "   - Prefer in-place operations (`+=`, `*=`) when possible\n",
    "   - Understand contiguity for optimal performance\n",
    "   - Use appropriate data types to balance precision and memory\n",
    "\n",
    "2. **Performance Optimization**:\n",
    "   - Always use vectorized operations instead of loops\n",
    "   - Leverage broadcasting to avoid explicit tensor expansion\n",
    "   - Use `torch.compile()` for performance-critical operations\n",
    "   - Consider memory access patterns for cache efficiency\n",
    "\n",
    "3. **Neural Network Design**:\n",
    "   - Use proper initialization schemes (Xavier, Kaiming)\n",
    "   - Understand dimension manipulation for multi-head attention\n",
    "   - Apply efficient masking techniques for sequence processing\n",
    "   - Optimize batch processing for memory efficiency\n",
    "\n",
    "4. **Debugging and Analysis**:\n",
    "   - Monitor tensor shapes throughout operations\n",
    "   - Understand broadcasting rules to avoid shape errors\n",
    "   - Use memory profiling for large-scale applications\n",
    "   - Validate numerical stability across data types\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Practice**: Implement the examples with your own data\n",
    "- **Experiment**: Try different initialization schemes and measure their impact\n",
    "- **Optimize**: Apply performance techniques to your existing code\n",
    "- **Build**: Create more complex neural network architectures\n",
    "- **Move Forward**: Progress to automatic differentiation and gradient computation\n",
    "\n",
    "This comprehensive foundation in tensor operations will serve as the basis for all advanced PyTorch development!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}