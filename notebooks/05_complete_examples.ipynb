{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": "<a href=\"https://colab.research.google.com/github/adiel2012/pythorch-for-deeplearning/blob/main/notebooks/05_complete_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Chapter 5: Complete Examples and Projects\n",
    "\n",
    "This notebook contains end-to-end implementations of common deep learning projects.\n",
    "\n",
    "## Projects Covered\n",
    "1. **Image Classification**: Complete CNN training pipeline\n",
    "2. **Regression**: Neural network for continuous prediction\n",
    "3. **Text Classification**: Simple NLP with embeddings\n",
    "4. **Autoencoder**: Dimensionality reduction and reconstruction\n",
    "5. **GAN**: Simple Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install and import necessary libraries\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    !pip install torch torchvision torchaudio\n",
    "    import torch\n",
    "    print(f\"PyTorch installed. Version: {torch.__version__}\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "\n",
    "# Set device and random seed\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project1"
   },
   "source": [
    "## Project 1: Complete Image Classification Pipeline\n",
    "\n",
    "End-to-end image classification with data loading, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "image_classification"
   },
   "outputs": [],
   "source": [
    "class ImageClassifier(nn.Module):\n",
    "    \"\"\"Complete image classification model\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ImageClassifier, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((4, 4))\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_classifier(model, train_loader, val_loader, num_epochs=10):\n",
    "    \"\"\"Complete training function with validation\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_classifier.pth')\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] '\n",
    "              f'Train Loss: {avg_train_loss:.4f} '\n",
    "              f'Train Acc: {train_acc:.2f}% '\n",
    "              f'Val Loss: {avg_val_loss:.4f} '\n",
    "              f'Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Create synthetic dataset (replace with real data)\n",
    "def create_synthetic_image_data(n_samples=1000, n_classes=5):\n",
    "    \"\"\"Create synthetic image data for demonstration\"\"\"\n",
    "    \n",
    "    # Generate random images\n",
    "    images = torch.randn(n_samples, 3, 32, 32)\n",
    "    \n",
    "    # Add some patterns based on class\n",
    "    labels = torch.randint(0, n_classes, (n_samples,))\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        # Add class-specific patterns\n",
    "        if label == 0:  # Red channel dominant\n",
    "            images[i, 0] += 0.5\n",
    "        elif label == 1:  # Green channel dominant\n",
    "            images[i, 1] += 0.5\n",
    "        elif label == 2:  # Blue channel dominant\n",
    "            images[i, 2] += 0.5\n",
    "        elif label == 3:  # Checkerboard pattern\n",
    "            for x in range(0, 32, 4):\n",
    "                for y in range(0, 32, 4):\n",
    "                    if (x//4 + y//4) % 2 == 0:\n",
    "                        images[i, :, x:x+2, y:y+2] += 0.3\n",
    "        else:  # Bright center\n",
    "            images[i, :, 12:20, 12:20] += 0.4\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Prepare data\n",
    "print(\"=== Image Classification Project ===\")\n",
    "print(\"Creating synthetic dataset...\")\n",
    "\n",
    "images, labels = create_synthetic_image_data(2000, 5)\n",
    "dataset = TensorDataset(images, labels)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Train model\n",
    "model = ImageClassifier(num_classes=5).to(device)\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "history = train_classifier(model, train_loader, val_loader, num_epochs=5)\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history['train_loss'], label='Training Loss', linewidth=2)\n",
    "ax1.plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(history['train_acc'], label='Training Accuracy', linewidth=2)\n",
    "ax2.plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {max(history['val_acc']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project2"
   },
   "source": [
    "## Project 2: Neural Network Regression\n",
    "\n",
    "Predicting continuous values with a feed-forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "regression_project"
   },
   "outputs": [],
   "source": [
    "class RegressionNet(nn.Module):\n",
    "    \"\"\"Neural network for regression tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size=1, dropout_rate=0.2):\n",
    "        super(RegressionNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Hidden layers\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer (no activation for regression)\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def train_regressor(model, train_loader, val_loader, num_epochs=50):\n",
    "    \"\"\"Train regression model\"\"\"\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_r2': [], 'val_r2': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds, train_targets = [], []\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(outputs.cpu().detach().numpy())\n",
    "            train_targets.extend(targets.cpu().detach().numpy())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds, val_targets = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        # Calculate R² scores\n",
    "        train_r2 = r2_score(train_targets, train_preds)\n",
    "        val_r2 = r2_score(val_targets, val_preds)\n",
    "        \n",
    "        # Update history\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_r2'].append(train_r2)\n",
    "        history['val_r2'].append(val_r2)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] '\n",
    "                  f'Train Loss: {avg_train_loss:.6f} '\n",
    "                  f'Val Loss: {avg_val_loss:.6f} '\n",
    "                  f'Train R²: {train_r2:.4f} '\n",
    "                  f'Val R²: {val_r2:.4f}')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Generate regression data\n",
    "print(\"=== Neural Network Regression Project ===\")\n",
    "print(\"Generating regression dataset...\")\n",
    "\n",
    "# Create synthetic regression data\n",
    "X, y = make_regression(n_samples=2000, n_features=20, n_informative=15, \n",
    "                      noise=0.1, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.FloatTensor(y_val).unsqueeze(1)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Input features: {X_train.shape[1]}\")\n",
    "\n",
    "# Create and train model\n",
    "model = RegressionNet(\n",
    "    input_size=X_train.shape[1],\n",
    "    hidden_sizes=[128, 64, 32],\n",
    "    output_size=1,\n",
    "    dropout_rate=0.2\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "history = train_regressor(model, train_loader, val_loader, num_epochs=50)\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_preds.extend(outputs.cpu().numpy())\n",
    "        test_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "# Calculate final metrics\n",
    "test_r2 = r2_score(test_targets, test_preds)\n",
    "test_mse = mean_squared_error(test_targets, test_preds)\n",
    "\n",
    "print(f\"\\nFinal Test Results:\")\n",
    "print(f\"R² Score: {test_r2:.4f}\")\n",
    "print(f\"MSE: {test_mse:.6f}\")\n",
    "print(f\"RMSE: {np.sqrt(test_mse):.6f}\")\n",
    "\n",
    "# Plot results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(history['train_loss'], label='Training Loss')\n",
    "ax1.plot(history['val_loss'], label='Validation Loss')\n",
    "ax1.set_title('Loss Curves')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# R² curves\n",
    "ax2.plot(history['train_r2'], label='Training R²')\n",
    "ax2.plot(history['val_r2'], label='Validation R²')\n",
    "ax2.set_title('R² Score Curves')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('R² Score')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Predictions vs actual\n",
    "ax3.scatter(test_targets, test_preds, alpha=0.6)\n",
    "ax3.plot([min(test_targets), max(test_targets)], \n",
    "         [min(test_targets), max(test_targets)], 'r--', linewidth=2)\n",
    "ax3.set_title(f'Predictions vs Actual (R² = {test_r2:.4f})')\n",
    "ax3.set_xlabel('Actual Values')\n",
    "ax3.set_ylabel('Predicted Values')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = np.array(test_targets).flatten() - np.array(test_preds).flatten()\n",
    "ax4.scatter(test_preds, residuals, alpha=0.6)\n",
    "ax4.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "ax4.set_title('Residual Plot')\n",
    "ax4.set_xlabel('Predicted Values')\n",
    "ax4.set_ylabel('Residuals')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project3"
   },
   "source": [
    "## Project 3: Simple Autoencoder\n",
    "\n",
    "Dimensionality reduction and data reconstruction using autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoencoder_project"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"Simple autoencoder for dimensionality reduction\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, encoding_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size // 2, input_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size // 4, encoding_size),\n",
    "            nn.ReLU()  # Ensure positive encoding\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_size, input_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size // 4, input_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size // 2, input_size),\n",
    "            nn.Tanh()  # Output in [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "def train_autoencoder(model, dataloader, num_epochs=100):\n",
    "    \"\"\"Train autoencoder\"\"\"\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for data in dataloader:\n",
    "            inputs = data[0].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)  # Reconstruct input\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Create synthetic high-dimensional data\n",
    "print(\"=== Autoencoder Project ===\")\n",
    "print(\"Generating high-dimensional data...\")\n",
    "\n",
    "# Generate data with some underlying structure\n",
    "n_samples = 1000\n",
    "n_features = 100\n",
    "encoding_size = 10\n",
    "\n",
    "# Create data with intrinsic lower dimensionality\n",
    "np.random.seed(42)\n",
    "latent_data = np.random.randn(n_samples, encoding_size)\n",
    "mixing_matrix = np.random.randn(encoding_size, n_features)\n",
    "high_dim_data = latent_data @ mixing_matrix\n",
    "high_dim_data += 0.1 * np.random.randn(n_samples, n_features)  # Add noise\n",
    "\n",
    "# Normalize data to [-1, 1]\n",
    "scaler = StandardScaler()\n",
    "high_dim_data = scaler.fit_transform(high_dim_data)\n",
    "high_dim_data = np.tanh(high_dim_data)  # Squash to [-1, 1]\n",
    "\n",
    "# Create dataset\n",
    "data_tensor = torch.FloatTensor(high_dim_data)\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"Data shape: {high_dim_data.shape}\")\n",
    "print(f\"Encoding dimension: {encoding_size}\")\n",
    "print(f\"Compression ratio: {n_features/encoding_size:.1f}:1\")\n",
    "\n",
    "# Create and train autoencoder\n",
    "autoencoder = Autoencoder(n_features, encoding_size).to(device)\n",
    "print(f\"\\nAutoencoder parameters: {sum(p.numel() for p in autoencoder.parameters()):,}\")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "losses = train_autoencoder(autoencoder, dataloader, num_epochs=100)\n",
    "\n",
    "# Test reconstruction\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    sample_data = data_tensor[:100].to(device)\n",
    "    reconstructed = autoencoder(sample_data)\n",
    "    encoded_rep = autoencoder.encode(sample_data)\n",
    "    \n",
    "    reconstruction_error = F.mse_loss(reconstructed, sample_data).item()\n",
    "    print(f\"\\nReconstruction MSE: {reconstruction_error:.6f}\")\n",
    "\n",
    "# Visualizations\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Training loss\n",
    "ax1.plot(losses, linewidth=2)\n",
    "ax1.set_title('Autoencoder Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Original vs reconstructed (first sample)\n",
    "sample_idx = 0\n",
    "original = sample_data[sample_idx].cpu().numpy()\n",
    "recon = reconstructed[sample_idx].cpu().numpy()\n",
    "\n",
    "ax2.plot(original[:50], label='Original', linewidth=2, alpha=0.7)\n",
    "ax2.plot(recon[:50], label='Reconstructed', linewidth=2, alpha=0.7)\n",
    "ax2.set_title(f'Sample Reconstruction (first 50 features)')\n",
    "ax2.set_xlabel('Feature Index')\n",
    "ax2.set_ylabel('Value')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Encoding space (2D projection using PCA if needed)\n",
    "encodings = encoded_rep.cpu().numpy()\n",
    "if encoding_size >= 2:\n",
    "    ax3.scatter(encodings[:, 0], encodings[:, 1], alpha=0.6, s=20)\n",
    "    ax3.set_title('Encoding Space (first 2 dimensions)')\n",
    "    ax3.set_xlabel('Encoding Dim 1')\n",
    "    ax3.set_ylabel('Encoding Dim 2')\n",
    "    ax3.grid(True, alpha=0.3)\nelse:\n",
    "    ax3.hist(encodings[:, 0], bins=30, alpha=0.7)\n",
    "    ax3.set_title('Encoding Distribution')\n",
    "    ax3.set_xlabel('Encoding Value')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Reconstruction scatter plot\n",
    "original_flat = sample_data.cpu().numpy().flatten()\n",
    "reconstructed_flat = reconstructed.cpu().numpy().flatten()\n",
    "\n",
    "ax4.scatter(original_flat, reconstructed_flat, alpha=0.3, s=1)\n",
    "ax4.plot([original_flat.min(), original_flat.max()], \n",
    "         [original_flat.min(), original_flat.max()], 'r--', linewidth=2)\n",
    "ax4.set_title('Original vs Reconstructed Values')\n",
    "ax4.set_xlabel('Original Values')\n",
    "ax4.set_ylabel('Reconstructed Values')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal reconstruction error: {reconstruction_error:.6f}\")\nprint(f\"Compression achieved: {n_features} → {encoding_size} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project4"
   },
   "source": [
    "## Project 4: Simple GAN\n",
    "\n",
    "Basic Generative Adversarial Network for generating synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gan_project"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator network for GAN\"\"\"\n",
    "    \n",
    "    def __init__(self, noise_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            \n",
    "            nn.Linear(128, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            \n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Tanh()  # Output in [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator network for GAN\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()  # Probability [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def train_gan(generator, discriminator, dataloader, num_epochs=200):\n",
    "    \"\"\"Train GAN with alternating updates\"\"\"\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Optimizers\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Training history\n",
    "    history = {'d_loss': [], 'g_loss': [], 'd_acc': []}\n",
    "    \n",
    "    noise_dim = 100\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_d_loss = 0.0\n",
    "        epoch_g_loss = 0.0\n",
    "        epoch_d_acc = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for real_data in dataloader:\n",
    "            batch_size = real_data[0].size(0)\n",
    "            real_data = real_data[0].to(device)\n",
    "            \n",
    "            # Labels\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            \n",
    "            # =====================\n",
    "            # Train Discriminator\n",
    "            # =====================\n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            # Real data\n",
    "            d_real_output = discriminator(real_data)\n",
    "            d_real_loss = criterion(d_real_output, real_labels)\n",
    "            \n",
    "            # Fake data\n",
    "            noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "            fake_data = generator(noise)\n",
    "            d_fake_output = discriminator(fake_data.detach())\n",
    "            d_fake_loss = criterion(d_fake_output, fake_labels)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            # ==================\n",
    "            # Train Generator\n",
    "            # ==================\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate fake data and try to fool discriminator\n",
    "            noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "            fake_data = generator(noise)\n",
    "            d_fake_output = discriminator(fake_data)\n",
    "            g_loss = criterion(d_fake_output, real_labels)  # Want discriminator to think it's real\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            epoch_d_loss += d_loss.item()\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            \n",
    "            # Discriminator accuracy\n",
    "            d_real_acc = (d_real_output > 0.5).float().mean().item()\n",
    "            d_fake_acc = (d_fake_output <= 0.5).float().mean().item()\n",
    "            epoch_d_acc += (d_real_acc + d_fake_acc) / 2\n",
    "            \n",
    "            num_batches += 1\n",
    "        \n",
    "        # Update history\n",
    "        history['d_loss'].append(epoch_d_loss / num_batches)\n",
    "        history['g_loss'].append(epoch_g_loss / num_batches)\n",
    "        history['d_acc'].append(epoch_d_acc / num_batches)\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] '\n",
    "                  f'D Loss: {history[\"d_loss\"][-1]:.4f} '\n",
    "                  f'G Loss: {history[\"g_loss\"][-1]:.4f} '\n",
    "                  f'D Acc: {history[\"d_acc\"][-1]:.4f}')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Create target distribution (mixture of Gaussians)\n",
    "print(\"=== Simple GAN Project ===\")\n",
    "print(\"Creating target distribution...\")\n",
    "\n",
    "def create_target_distribution(n_samples=2000, data_dim=2):\n",
    "    \"\"\"Create a mixture of Gaussians as target distribution\"\"\"\n",
    "    \n",
    "    # Three Gaussian clusters\n",
    "    cluster1 = np.random.multivariate_normal([2, 2], [[0.5, 0], [0, 0.5]], n_samples // 3)\n",
    "    cluster2 = np.random.multivariate_normal([-2, 2], [[0.5, 0], [0, 0.5]], n_samples // 3)\n",
    "    cluster3 = np.random.multivariate_normal([0, -2], [[0.5, 0], [0, 0.5]], n_samples // 3)\n",
    "    \n",
    "    data = np.vstack([cluster1, cluster2, cluster3])\n",
    "    \n",
    "    # Add more dimensions if needed\n",
    "    if data_dim > 2:\n",
    "        extra_dims = np.random.randn(data.shape[0], data_dim - 2) * 0.1\n",
    "        data = np.hstack([data, extra_dims])\n",
    "    \n",
    "    # Normalize to [-1, 1]\n",
    "    data = np.tanh(data / 2)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate target data\n",
    "data_dim = 2  # Keep 2D for easy visualization\n",
    "target_data = create_target_distribution(2000, data_dim)\n",
    "\n",
    "print(f\"Target data shape: {target_data.shape}\")\n",
    "print(f\"Data range: [{target_data.min():.2f}, {target_data.max():.2f}]\")\n",
    "\n",
    "# Create dataset\n",
    "target_tensor = torch.FloatTensor(target_data)\n",
    "dataset = TensorDataset(target_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create GAN\n",
    "noise_dim = 100\n",
    "generator = Generator(noise_dim, data_dim).to(device)\n",
    "discriminator = Discriminator(data_dim).to(device)\n",
    "\n",
    "print(f\"\\nGenerator parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
    "\n",
    "# Train GAN\n",
    "print(\"\\nStarting GAN training...\")\n",
    "history = train_gan(generator, discriminator, dataloader, num_epochs=200)\n",
    "\n",
    "# Generate samples\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(1000, noise_dim).to(device)\n",
    "    generated_data = generator(noise).cpu().numpy()\n",
    "\n",
    "print(f\"\\nGenerated data shape: {generated_data.shape}\")\n",
    "print(f\"Generated data range: [{generated_data.min():.2f}, {generated_data.max():.2f}]\")\n",
    "\n",
    "# Visualizations\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Training curves\n",
    "ax1.plot(history['d_loss'], label='Discriminator Loss', linewidth=2)\n",
    "ax1.plot(history['g_loss'], label='Generator Loss', linewidth=2)\n",
    "ax1.set_title('GAN Training Losses')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Discriminator accuracy\n",
    "ax2.plot(history['d_acc'], linewidth=2, color='green')\n",
    "ax2.axhline(y=0.5, color='red', linestyle='--', linewidth=2, label='Random Guess')\n",
    "ax2.set_title('Discriminator Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Data distribution comparison (2D)\n",
    "if data_dim == 2:\n",
    "    ax3.scatter(target_data[:, 0], target_data[:, 1], alpha=0.6, s=20, label='Real Data')\n",
    "    ax3.set_title('Real Data Distribution')\n",
    "    ax3.set_xlabel('Dimension 1')\n",
    "    ax3.set_ylabel('Dimension 2')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax4.scatter(generated_data[:, 0], generated_data[:, 1], alpha=0.6, s=20, \n",
    "               color='orange', label='Generated Data')\n",
    "    ax4.set_title('Generated Data Distribution')\n",
    "    ax4.set_xlabel('Dimension 1')\n",
    "    ax4.set_ylabel('Dimension 2')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\nelse:\n",
    "    # For higher dimensions, show marginal distributions\n",
    "    ax3.hist(target_data[:, 0], bins=30, alpha=0.7, label='Real Data')\n",
    "    ax3.hist(generated_data[:, 0], bins=30, alpha=0.7, label='Generated Data')\n",
    "    ax3.set_title('Marginal Distribution (Dim 1)')\n",
    "    ax3.set_xlabel('Value')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax4.hist(target_data[:, 1], bins=30, alpha=0.7, label='Real Data')\n",
    "    ax4.hist(generated_data[:, 1], bins=30, alpha=0.7, label='Generated Data')\n",
    "    ax4.set_title('Marginal Distribution (Dim 2)')\n",
    "    ax4.set_xlabel('Value')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal discriminator accuracy: {history['d_acc'][-1]:.4f}\")\nprint(\"Note: Ideal discriminator accuracy should be around 0.5 (can't distinguish real from fake)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## Summary: Complete Deep Learning Projects\n",
    "\n",
    "In this comprehensive notebook, we implemented five complete deep learning projects:\n",
    "\n",
    "### 1. Image Classification Pipeline\n",
    "- **Architecture**: Multi-block CNN with batch normalization and dropout\n",
    "- **Features**: Data augmentation, validation monitoring, learning rate scheduling\n",
    "- **Key Concepts**: Transfer learning potential, regularization techniques\n",
    "\n",
    "### 2. Neural Network Regression\n",
    "- **Architecture**: Deep feed-forward network with batch normalization\n",
    "- **Evaluation**: R² score, MSE, residual analysis\n",
    "- **Applications**: Continuous value prediction, function approximation\n",
    "\n",
    "### 3. Autoencoder for Dimensionality Reduction\n",
    "- **Architecture**: Symmetric encoder-decoder with bottleneck\n",
    "- **Applications**: Data compression, denoising, anomaly detection\n",
    "- **Insights**: Learned representations, reconstruction quality\n",
    "\n",
    "### 4. Simple Generative Adversarial Network\n",
    "- **Architecture**: Generator and discriminator in adversarial training\n",
    "- **Concepts**: Minimax game, Nash equilibrium, mode collapse\n",
    "- **Applications**: Data generation, distribution matching\n",
    "\n",
    "### Key Implementation Patterns\n",
    "\n",
    "1. **Model Design**:\n",
    "   - Modular architecture with clear forward pass\n",
    "   - Appropriate activation functions and normalization\n",
    "   - Regularization techniques (dropout, weight decay)\n",
    "\n",
    "2. **Training Loop**:\n",
    "   - Proper gradient management (zero_grad, backward, step)\n",
    "   - Validation monitoring and early stopping\n",
    "   - Learning rate scheduling\n",
    "\n",
    "3. **Data Handling**:\n",
    "   - Efficient data loading with DataLoader\n",
    "   - Appropriate train/validation/test splits\n",
    "   - Data preprocessing and normalization\n",
    "\n",
    "4. **Evaluation and Visualization**:\n",
    "   - Comprehensive metrics for different tasks\n",
    "   - Training curve monitoring\n",
    "   - Result visualization and interpretation\n",
    "\n",
    "### Best Practices Demonstrated\n",
    "\n",
    "- **Device Management**: Proper GPU utilization\n",
    "- **Reproducibility**: Random seed setting\n",
    "- **Model Saving**: Checkpoint best models\n",
    "- **Monitoring**: Track multiple metrics\n",
    "- **Regularization**: Prevent overfitting\n",
    "- **Architecture Design**: Match model to task\n",
    "\n",
    "### Next Steps for Real Projects\n",
    "\n",
    "1. **Use Real Datasets**: Replace synthetic data with domain-specific datasets\n",
    "2. **Hyperparameter Tuning**: Implement grid search or Bayesian optimization\n",
    "3. **Advanced Architectures**: Explore ResNet, Transformers, etc.\n",
    "4. **Transfer Learning**: Use pre-trained models\n",
    "5. **Production Deployment**: Model serving and monitoring\n",
    "6. **Advanced Training**: Mixed precision, distributed training\n",
    "\n",
    "These projects provide a solid foundation for building more complex deep learning applications!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM...",
   "collapsed_sections": [],
   "name": "05_complete_examples.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}