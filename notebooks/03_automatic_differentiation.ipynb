{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/your-username/pytorch-for-deeplearning/blob/main/notebooks/03_automatic_differentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# Chapter 3: Automatic Differentiation\n",
    "\n",
    "This notebook covers PyTorch's automatic differentiation (autograd) system - the foundation of neural network training.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand gradient computation with `requires_grad`\n",
    "- Explore the computational graph\n",
    "- Learn backpropagation mechanics\n",
    "- Handle gradient accumulation and zeroing\n",
    "- Work with higher-order derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install and import necessary libraries\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    !pip install torch torchvision torchaudio\n",
    "    import torch\n",
    "    print(f\"PyTorch installed. Version: {torch.__version__}\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph\n",
    "\n",
    "# Set device and random seed\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Try to install graphviz for computational graph visualization\n",
    "try:\n",
    "    !pip install graphviz\n",
    "    print(\"Graphviz installed for computational graph visualization\")\n",
    "except:\n",
    "    print(\"Could not install graphviz - graph visualization will be limited\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "basics"
   },
   "source": [
    "## 1. Gradient Computation Basics\n",
    "\n",
    "Understanding `requires_grad` and basic gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "basic_gradients"
   },
   "outputs": [],
   "source": [
    "print(\"=== Basic Gradient Computation ===\")\n",
    "\n",
    "# Create tensors with gradient tracking\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "print(f\"x = {x}, requires_grad = {x.requires_grad}\")\n",
    "print(f\"y = {y}, requires_grad = {y.requires_grad}\")\n",
    "\n",
    "# Perform computations\n",
    "z = x**2 + 2*y + 1\n",
    "print(f\"\\nz = x² + 2y + 1 = {z}\")\n",
    "print(f\"z.requires_grad = {z.requires_grad}\")\n",
    "print(f\"z.grad_fn = {z.grad_fn}\")\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "print(f\"\\nAfter z.backward():\")\n",
    "print(f\"∂z/∂x = {x.grad} (expected: 2x = {2*x.item()})\")\n",
    "print(f\"∂z/∂y = {y.grad} (expected: 2)\")\n",
    "\n",
    "# Verify gradients manually\n",
    "print(f\"\\nManual verification:\")\n",
    "print(f\"∂z/∂x = ∂(x² + 2y + 1)/∂x = 2x = {2 * x.item()}\")\n",
    "print(f\"∂z/∂y = ∂(x² + 2y + 1)/∂y = 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "computational_graph"
   },
   "source": [
    "## 2. Computational Graph\n",
    "\n",
    "Understanding how PyTorch builds and uses the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "graph_exploration"
   },
   "outputs": [],
   "source": [
    "print(\"=== Computational Graph Exploration ===\")\n",
    "\n",
    "# Create a more complex computation\n",
    "a = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(2.0, requires_grad=True)\n",
    "c = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Build computation graph: d = (a + b) * c\n",
    "temp = a + b\n",
    "d = temp * c\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"c = {c}\")\n",
    "print(f\"temp = a + b = {temp}\")\n",
    "print(f\"d = temp * c = {d}\")\n",
    "\n",
    "print(f\"\\nGrad functions:\")\n",
    "print(f\"temp.grad_fn = {temp.grad_fn}\")\n",
    "print(f\"d.grad_fn = {d.grad_fn}\")\n",
    "\n",
    "# Inspect the graph structure\n",
    "print(f\"\\nGraph structure:\")\n",
    "print(f\"d.grad_fn.next_functions = {d.grad_fn.next_functions}\")\n",
    "\n",
    "# Compute gradients\n",
    "d.backward()\n",
    "\n",
    "print(f\"\\nGradients:\")\n",
    "print(f\"∂d/∂a = {a.grad} (expected: c = {c.item()})\")\n",
    "print(f\"∂d/∂b = {b.grad} (expected: c = {c.item()})\")\n",
    "print(f\"∂d/∂c = {c.grad} (expected: a+b = {a.item() + b.item()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_graph"
   },
   "outputs": [],
   "source": [
    "# Function to visualize computational graph (simplified)\n",
    "def visualize_graph_simple():\n",
    "    \"\"\"Create a simple visualization of computational graph\"\"\"\n",
    "    print(\"\\n=== Computational Graph Visualization ===\")\n",
    "    print(\"\")\n",
    "    print(\"  a(1.0)    b(2.0)\")\n",
    "    print(\"     \\\\      /\")\n",
    "    print(\"      \\\\    /\")\n",
    "    print(\"       \\\\  /\")\n",
    "    print(\"        +   ← AddBackward\")\n",
    "        print(\"        |\")\n",
    "    print(\"    temp(3.0)\")\n",
    "    print(\"        |\")\n",
    "    print(\"        |  c(3.0)\")\n",
    "    print(\"        |    /\")\n",
    "    print(\"        |   /\")\n",
    "    print(\"        |  /\")\n",
    "    print(\"        * ← MulBackward\")\n",
    "    print(\"        |\")\n",
    "    print(\"      d(9.0)\")\n",
    "    print(\"\")\n",
    "    print(\"Backward pass:\")\n",
    "    print(\"1. d.backward() starts with gradient 1.0\")\n",
    "    print(\"2. MulBackward: grad_temp = 1.0 * c, grad_c = 1.0 * temp\")\n",
    "    print(\"3. AddBackward: grad_a = grad_temp, grad_b = grad_temp\")\n",
    "\n",
    "visualize_graph_simple()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "backprop_mechanics"
   },
   "source": [
    "## 3. Backpropagation Mechanics\n",
    "\n",
    "Understanding how gradients flow backwards through the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "backprop_step_by_step"
   },
   "outputs": [],
   "source": [
    "print(\"=== Step-by-step Backpropagation ===\")\n",
    "\n",
    "# Create a simple neural network computation\n",
    "# f(x, w, b) = σ(w * x + b) where σ is sigmoid\n",
    "x = torch.tensor(0.5, requires_grad=True)\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(-1.0, requires_grad=True)\n",
    "\n",
    "print(f\"Input: x = {x.item()}\")\n",
    "print(f\"Weight: w = {w.item()}\")\n",
    "print(f\"Bias: b = {b.item()}\")\n",
    "\n",
    "# Forward pass\n",
    "linear = w * x + b\n",
    "output = torch.sigmoid(linear)\n",
    "\n",
    "print(f\"\\nForward pass:\")\n",
    "print(f\"linear = w*x + b = {linear.item():.4f}\")\n",
    "print(f\"output = σ(linear) = {output.item():.4f}\")\n",
    "\n",
    "# Backward pass\n",
    "output.backward()\n",
    "\n",
    "print(f\"\\nBackward pass gradients:\")\n",
    "print(f\"∂output/∂b = {b.grad.item():.4f}\")\n",
    "print(f\"∂output/∂w = {w.grad.item():.4f}\")\n",
    "print(f\"∂output/∂x = {x.grad.item():.4f}\")\n",
    "\n",
    "# Manual gradient computation for verification\n",
    "print(f\"\\nManual verification:\")\n",
    "sigmoid_grad = output.item() * (1 - output.item())  # d/dx σ(x) = σ(x)(1-σ(x))\n",
    "print(f\"σ'(linear) = σ(linear)(1-σ(linear)) = {sigmoid_grad:.4f}\")\n",
    "print(f\"∂output/∂b = σ'(linear) * 1 = {sigmoid_grad:.4f}\")\n",
    "print(f\"∂output/∂w = σ'(linear) * x = {sigmoid_grad * x.item():.4f}\")\n",
    "print(f\"∂output/∂x = σ'(linear) * w = {sigmoid_grad * w.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gradient_management"
   },
   "source": [
    "## 4. Gradient Management\n",
    "\n",
    "Learning how to manage gradients: accumulation, zeroing, and detaching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gradient_accumulation"
   },
   "outputs": [],
   "source": [
    "print(\"=== Gradient Accumulation ===\")\n",
    "\n",
    "# Gradients accumulate by default\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# First computation and backward\n",
    "y1 = x**2\n",
    "y1.backward()\n",
    "print(f\"After first backward (y1 = x²): x.grad = {x.grad}\")\n",
    "\n",
    "# Second computation and backward (gradients accumulate!)\n",
    "y2 = x**3\n",
    "y2.backward()\n",
    "print(f\"After second backward (y2 = x³): x.grad = {x.grad}\")\n",
    "print(f\"Expected: 2x + 3x² = {2*x.item()} + {3*x.item()**2} = {2*x.item() + 3*x.item()**2}\")\n",
    "\n",
    "# Zero gradients before next computation\n",
    "x.grad.zero_()\n",
    "print(f\"After zeroing gradients: x.grad = {x.grad}\")\n",
    "\n",
    "# Third computation\n",
    "y3 = 2 * x + 5\n",
    "y3.backward()\n",
    "print(f\"After third backward (y3 = 2x + 5): x.grad = {x.grad}\")\n",
    "print(f\"Expected: 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "detaching_gradients"
   },
   "outputs": [],
   "source": [
    "print(\"\\n=== Detaching from Computational Graph ===\")\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2\n",
    "\n",
    "print(f\"y = x² = {y}\")\n",
    "print(f\"y.requires_grad = {y.requires_grad}\")\n",
    "\n",
    "# Detach y from the graph\n",
    "y_detached = y.detach()\n",
    "print(f\"\\ny_detached = {y_detached}\")\n",
    "print(f\"y_detached.requires_grad = {y_detached.requires_grad}\")\n",
    "\n",
    "# Using detached tensor\n",
    "z1 = y * 2  # This maintains the graph\n",
    "z2 = y_detached * 2  # This breaks the graph\n",
    "\n",
    "print(f\"\\nz1 = y * 2, z1.requires_grad = {z1.requires_grad}\")\n",
    "print(f\"z2 = y_detached * 2, z2.requires_grad = {z2.requires_grad}\")\n",
    "\n",
    "# Backward through z1 works\n",
    "z1.backward()\n",
    "print(f\"\\nAfter z1.backward(): x.grad = {x.grad}\")\n",
    "\n",
    "# Reset gradient\n",
    "x.grad.zero_()\n",
    "\n",
    "# Backward through z2 won't affect x\n",
    "try:\n",
    "    z2.backward()\n",
    "    print(f\"After z2.backward(): x.grad = {x.grad}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error with z2.backward(): {e}\")\n",
    "    print(\"z2 doesn't require gradients, so backward() fails\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "context_managers"
   },
   "source": [
    "## 5. Context Managers for Gradient Control\n",
    "\n",
    "Using `torch.no_grad()` and `torch.enable_grad()` for gradient control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "no_grad_context"
   },
   "outputs": [],
   "source": [
    "print(\"=== Context Managers ===\")\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# Normal computation with gradients\n",
    "y1 = x**2\n",
    "print(f\"Normal: y1 = x² = {y1}, requires_grad = {y1.requires_grad}\")\n",
    "\n",
    "# Computation without gradients\n",
    "with torch.no_grad():\n",
    "    y2 = x**2\n",
    "    print(f\"No grad: y2 = x² = {y2}, requires_grad = {y2.requires_grad}\")\n",
    "\n",
    "# Inference mode (PyTorch 1.9+)\n",
    "with torch.inference_mode():\n",
    "    y3 = x**2\n",
    "    print(f\"Inference: y3 = x² = {y3}, requires_grad = {y3.requires_grad}\")\n",
    "\n",
    "print(f\"\\nMemory efficiency comparison:\")\n",
    "print(f\"y1.grad_fn = {y1.grad_fn} (stores computation graph)\")\n",
    "print(f\"y2.grad_fn = {y2.grad_fn} (no computation graph)\")\n",
    "print(f\"y3.grad_fn = {y3.grad_fn} (no computation graph)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "higher_order"
   },
   "source": [
    "## 6. Higher-Order Derivatives\n",
    "\n",
    "Computing second and higher-order derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "second_derivatives"
   },
   "outputs": [],
   "source": [
    "print(\"=== Higher-Order Derivatives ===\")\n",
    "\n",
    "# Function: f(x) = x³ - 2x² + x + 1\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**3 - 2*x**2 + x + 1\n",
    "\n",
    "print(f\"f(x) = x³ - 2x² + x + 1\")\n",
    "print(f\"f({x.item()}) = {y.item()}\")\n",
    "\n",
    "# First derivative\n",
    "grad_1 = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "print(f\"\\nFirst derivative: f'(x) = 3x² - 4x + 1\")\n",
    "print(f\"f'({x.item()}) = {grad_1.item()}\")\n",
    "print(f\"Expected: 3({x.item()})² - 4({x.item()}) + 1 = {3*x.item()**2 - 4*x.item() + 1}\")\n",
    "\n",
    "# Second derivative\n",
    "grad_2 = torch.autograd.grad(grad_1, x, create_graph=True)[0]\n",
    "print(f\"\\nSecond derivative: f''(x) = 6x - 4\")\n",
    "print(f\"f''({x.item()}) = {grad_2.item()}\")\n",
    "print(f\"Expected: 6({x.item()}) - 4 = {6*x.item() - 4}\")\n",
    "\n",
    "# Third derivative\n",
    "grad_3 = torch.autograd.grad(grad_2, x, create_graph=True)[0]\n",
    "print(f\"\\nThird derivative: f'''(x) = 6\")\n",
    "print(f\"f'''({x.item()}) = {grad_3.item()}\")\n",
    "print(f\"Expected: 6\")\n",
    "\n",
    "# Fourth derivative (should be zero)\n",
    "grad_4 = torch.autograd.grad(grad_3, x)[0]\n",
    "print(f\"\\nFourth derivative: f''''(x) = 0\")\n",
    "print(f\"f''''({x.item()}) = {grad_4.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gradient_flow"
   },
   "source": [
    "## 7. Gradient Flow Analysis\n",
    "\n",
    "Understanding how gradients flow through different operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze_gradient_flow"
   },
   "outputs": [],
   "source": [
    "print(\"=== Gradient Flow Analysis ===\")\n",
    "\n",
    "def analyze_gradients(func, func_name, x_range=(-3, 3)):\n",
    "    \"\"\"Analyze gradient behavior of a function\"\"\"\n",
    "    x = torch.linspace(x_range[0], x_range[1], 100, requires_grad=True)\n",
    "    y = func(x)\n",
    "    \n",
    "    # Compute gradients\n",
    "    grad_outputs = torch.ones_like(y)\n",
    "    grads = torch.autograd.grad(outputs=y, inputs=x, \n",
    "                               grad_outputs=grad_outputs,\n",
    "                               create_graph=True)[0]\n",
    "    \n",
    "    # Statistics\n",
    "    grad_mean = grads.mean().item()\n",
    "    grad_std = grads.std().item()\n",
    "    grad_max = grads.max().item()\n",
    "    grad_min = grads.min().item()\n",
    "    zero_grads = (grads == 0).sum().item()\n",
    "    \n",
    "    print(f\"\\n{func_name}:\")\n",
    "    print(f\"  Gradient mean: {grad_mean:.4f}\")\n",
    "    print(f\"  Gradient std:  {grad_std:.4f}\")\n",
    "    print(f\"  Gradient range: [{grad_min:.4f}, {grad_max:.4f}]\")\n",
    "    print(f\"  Zero gradients: {zero_grads}\")\n",
    "    \n",
    "    return x.detach(), y.detach(), grads.detach()\n",
    "\n",
    "# Analyze different functions\n",
    "functions = [\n",
    "    (lambda x: torch.relu(x), \"ReLU\"),\n",
    "    (lambda x: torch.sigmoid(x), \"Sigmoid\"),\n",
    "    (lambda x: torch.tanh(x), \"Tanh\"),\n",
    "    (lambda x: x**2, \"Quadratic\"),\n",
    "    (lambda x: torch.sin(x), \"Sine\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "for func, name in functions:\n",
    "    x, y, grad = analyze_gradients(func, name)\n",
    "    results.append((x, y, grad, name))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (x, y, grad, name) in enumerate(results):\n",
    "    if i < len(axes):\n",
    "        ax = axes[i]\n",
    "        ax.plot(x.numpy(), y.numpy(), label=f'{name}(x)', linewidth=2)\n",
    "        ax.plot(x.numpy(), grad.numpy(), label=f\"{name}'(x)\", linewidth=2, alpha=0.7)\n",
    "        ax.set_title(f'{name} and its Gradient')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "        ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# Remove empty subplot\n",
    "if len(results) < len(axes):\n",
    "    fig.delaxes(axes[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercises"
   },
   "source": [
    "## 8. Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise1"
   },
   "outputs": [],
   "source": [
    "# Exercise 1: Implement gradient descent manually\n",
    "print(\"Exercise 1: Manual Gradient Descent\")\n",
    "print(\"Minimizing f(x) = (x - 3)² + 1\")\n",
    "\n",
    "# Initialize\n",
    "x = torch.tensor(0.0, requires_grad=True)\n",
    "learning_rate = 0.1\n",
    "num_steps = 20\n",
    "\n",
    "print(f\"Starting at x = {x.item():.4f}\")\n",
    "print(f\"True minimum at x = 3\")\n",
    "\n",
    "losses = []\n",
    "x_values = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Forward pass\n",
    "    loss = (x - 3)**2 + 1\n",
    "    losses.append(loss.item())\n",
    "    x_values.append(x.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update (no_grad to avoid building computation graph for update)\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad\n",
    "    \n",
    "    # Zero gradients\n",
    "    x.grad.zero_()\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step:2d}: x = {x.item():.4f}, loss = {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal: x = {x.item():.4f}, expected = 3.0000\")\n",
    "print(f\"Final loss = {losses[-1]:.6f}, expected = 1.0000\")\n",
    "\n",
    "# Plot convergence\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_values, losses, 'bo-', linewidth=2, markersize=6)\n",
    "plt.axhline(y=1.0, color='r', linestyle='--', label='True minimum')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs x')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(num_steps), losses, 'go-', linewidth=2, markersize=6)\n",
    "plt.axhline(y=1.0, color='r', linestyle='--', label='True minimum')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Step')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise2"
   },
   "outputs": [],
   "source": [
    "# Exercise 2: Chain rule verification\n",
    "print(\"\\nExercise 2: Chain Rule Verification\")\n",
    "print(\"Computing ∂h/∂x where h = g(f(x)), f(x) = x², g(u) = sin(u)\")\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# Composite function: h(x) = sin(x²)\n",
    "f_x = x**2  # f(x) = x²\n",
    "h = torch.sin(f_x)  # h = sin(f(x))\n",
    "\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"f(x) = x² = {f_x.item()}\")\n",
    "print(f\"h = sin(f(x)) = sin({f_x.item():.4f}) = {h.item():.4f}\")\n",
    "\n",
    "# Automatic differentiation\n",
    "h.backward()\n",
    "auto_grad = x.grad.item()\n",
    "\n",
    "print(f\"\\nAutomatic differentiation: ∂h/∂x = {auto_grad:.6f}\")\n",
    "\n",
    "# Manual chain rule: ∂h/∂x = (∂h/∂f) * (∂f/∂x)\n",
    "# ∂h/∂f = ∂sin(f)/∂f = cos(f)\n",
    "# ∂f/∂x = ∂(x²)/∂x = 2x\n",
    "dh_df = torch.cos(f_x).item()  # cos(x²)\n",
    "df_dx = 2 * x.item()  # 2x\n",
    "manual_grad = dh_df * df_dx\n",
    "\n",
    "print(f\"\\nManual chain rule:\")\n",
    "print(f\"∂h/∂f = cos(f) = cos({f_x.item():.4f}) = {dh_df:.6f}\")\n",
    "print(f\"∂f/∂x = 2x = 2({x.item()}) = {df_dx:.6f}\")\n",
    "print(f\"∂h/∂x = (∂h/∂f) * (∂f/∂x) = {dh_df:.6f} * {df_dx:.6f} = {manual_grad:.6f}\")\n",
    "\n",
    "print(f\"\\nVerification: {abs(auto_grad - manual_grad) < 1e-6}\")\n",
    "print(f\"Difference: {abs(auto_grad - manual_grad):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. **Basic Gradients**: Understanding `requires_grad` and gradient computation\n",
    "2. **Computational Graph**: How PyTorch builds and uses computation graphs\n",
    "3. **Backpropagation**: Step-by-step gradient flow through operations\n",
    "4. **Gradient Management**: Accumulation, zeroing, and detaching\n",
    "5. **Context Managers**: Controlling gradient computation with `torch.no_grad()`\n",
    "6. **Higher-Order Derivatives**: Computing second and third derivatives\n",
    "7. **Gradient Flow**: Analyzing how gradients behave in different functions\n",
    "8. **Manual Implementation**: Implementing gradient descent from scratch\n",
    "\n",
    "### Key Concepts\n",
    "- **Autograd**: PyTorch's automatic differentiation system\n",
    "- **Dynamic Graphs**: Computation graphs are built on-the-fly\n",
    "- **Gradient Accumulation**: Gradients add up unless explicitly zeroed\n",
    "- **Chain Rule**: Automatic application for complex compositions\n",
    "- **Memory Management**: Use `no_grad()` for inference to save memory\n",
    "\n",
    "### Best Practices\n",
    "1. Always zero gradients before backward pass in training loops\n",
    "2. Use `torch.no_grad()` for inference to save memory\n",
    "3. Be careful with in-place operations that can break gradients\n",
    "4. Use `detach()` when you need to stop gradient flow\n",
    "5. Monitor gradient magnitudes to detect vanishing/exploding gradients\n",
    "\n",
    "### Next Steps\n",
    "- Apply autograd to build and train neural networks\n",
    "- Learn about optimizers that use these gradients\n",
    "- Understand gradient-based optimization algorithms\n",
    "- Move on to the next notebook: Neural Network Building Blocks"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM...",
   "collapsed_sections": [],
   "name": "03_automatic_differentiation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}